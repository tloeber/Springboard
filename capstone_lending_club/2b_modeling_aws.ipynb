{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-packages-and-data\" data-toc-modified-id=\"Load-packages-and-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load packages and data</a></span></li><li><span><a href=\"#Predictive-models\" data-toc-modified-id=\"Predictive-models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Predictive models</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-with-RBF-Kernel\" data-toc-modified-id=\"SVM-with-RBF-Kernel-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>SVM with RBF-Kernel</a></span><ul class=\"toc-item\"><li><span><a href=\"#With-smaller-subset-of-data\" data-toc-modified-id=\"With-smaller-subset-of-data-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>With smaller subset of data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperopt\" data-toc-modified-id=\"Hyperopt-2.1.1.1\"><span class=\"toc-item-num\">2.1.1.1&nbsp;&nbsp;</span>Hyperopt</a></span></li><li><span><a href=\"#With-more-data\" data-toc-modified-id=\"With-more-data-2.1.1.2\"><span class=\"toc-item-num\">2.1.1.2&nbsp;&nbsp;</span>With more data</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "import missingno  # for visualizing missing data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    GridSearchCV, ShuffleSplit\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, \\\n",
    "    LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    "    precision_recall_curve, average_precision_score, f1_score, \\\n",
    "    roc_curve, auc, roc_auc_score, make_scorer,\\\n",
    "    accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# Adjust number of CPU cores to use\n",
    "# n_jobs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training and test set, incl. feature names \n",
    "\n",
    "# X_train = joblib.load('data_processed/X_train.joblib')\n",
    "# X_test = joblib.load('data_processed/X_test.joblib')\n",
    "# y_train = joblib.load('data_processed/y_train.joblib')\n",
    "# y_test = joblib.load('data_processed/y_test.joblib')\n",
    "X_train_small = joblib.load('data_processed/X_train_small.joblib')\n",
    "X_test_small = joblib.load('data_processed/X_test_small.joblib')\n",
    "y_train_small = joblib.load('data_processed/y_train_small.joblib')\n",
    "y_test_small = joblib.load('data_processed/y_test_small.joblib')\n",
    "feature_names = joblib.load('data_processed/feature_names')\n",
    "feature_names_small = joblib.load('data_processed/feature_names_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with RBF-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_1 = SVC(kernel='rbf', probability=False, \n",
    "              cache_size=5000, class_weight='balanced')\n",
    "# Parameters to search over\n",
    "param_grid = {'C': np.logspace(-3, 3, 8),\n",
    "              'gamma': np.logspace(-5, 1, 8)}\n",
    "# Define indices for validation split (instead of proper cross-validation)\n",
    "split_indices = ShuffleSplit(n_splits=1, test_size=.2, random_state=1)\n",
    "\n",
    "svm_rbf_gs_1 = GridSearchCV(svm_rbf_1, param_grid=param_grid,\n",
    "                          return_train_score=True,\n",
    "                          scoring='average_precision', cv=split_indices)\n",
    "svm_rbf_gs_1.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(svm_rbf_gs_1, 'saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "svm_rbf_gs_1 = joblib.load('saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of class\n",
    "y_pred_svm_rbf_1 = svm_rbf_gs_1.predict(X_test_small)\n",
    "\n",
    "# Distance from separating hyperplane\n",
    "y_pred_distance_svm_rbf_1 = svm_rbf_gs_1.decision_function(X_test_small)\n",
    "\n",
    "# Save results\n",
    "average_precision_1['SVM (RBF Kernel)'] = \\\n",
    "    average_precision_score(y_test_small, y_pred_distance_svm_rbf_1)\n",
    "\n",
    "classification_reports_1['SVM (RBF Kernel)'] = \\\n",
    "    classification_report(y_test_small, y_pred_svm_rbf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_best_result(svm_rbf_gs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_heatmap(svm_rbf_gs_1) #, x_digits=1, x_scientific_notation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_rbf_gs_1.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define hyperparameter search space\n",
    "space = {\n",
    "    'C': hp.uniform('C', 1E-10, 1E10),\n",
    "    'gamma': hp.uniform('gamma', 1E-5, 1E8) \n",
    "}\n",
    "\n",
    "# Function to minimize\n",
    "N_FOLDS=3\n",
    "N_JOBS=-2\n",
    "def objective(params, n_folds=N_FOLDS, n_jobs=N_JOBS):\n",
    "    \"\"\"Objective function to minimize\"\"\"\n",
    "    \n",
    "    # Instantiate estimator\n",
    "    clf = SVC(kernel='rbf', cache_size=6000, class_weight='balanced',\n",
    "              random_state=1, **params)\n",
    "    \n",
    "    # Cross-validation object\n",
    "    cv = StratifiedKFold(n_folds, random_state=1)\n",
    "    \n",
    "    # Compute average precision through CV\n",
    "    score = cross_val_score(clf, X_train_small, y_train_small, cv=cv,\n",
    "                            scoring='average_precision', n_jobs=n_jobs)\n",
    "    \n",
    "    # Return results\n",
    "    return {'loss': -score.mean(), 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# Minimize objective\n",
    "MAX_EVALS=4 \n",
    "best = fmin(objective, space, algo=tpe.suggest,\n",
    "            max_evals=MAX_EVALS, trials=trials)\n",
    "\n",
    "print ('Best parameters: {}'.format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values of the optimal parameters\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# Fit the model with the optimal hyperparamters\n",
    "clf.set_params(**best_params)\n",
    "clf.fit(X_train_small, y_train_small);\n",
    "\n",
    "# Score with the test data\n",
    "y_score = pipe.predict_proba(X_test_small)\n",
    "ap_score = average_precision_scorerecision_score(y_test_small, y_score[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_1score(X_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_rbf_1best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_1(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'objective':'binary:logistic', 'eval_metric':'map',\n",
    "#          'scale_pos_weight':5,  # Balance class weight\n",
    "#          'seed':0, 'learning_rate':0.1, 'n_estimators':1000,\n",
    "#          'max_depth':5, 'min_child_weight':1, 'gamma':0,\n",
    "#          'subsample':0.8, 'colsample_bytree':0.8,\n",
    "#          'nthread':4}\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':'map'\n",
    "}\n",
    "\n",
    "xgb_cv = xgb.train(params=params, dtrain=data_xgb_train, \n",
    "                num_boost_round=1000,\n",
    "                evals=[(data_xgb_test, \"test\")],\n",
    "                early_stopping_rounds=50) \n",
    "#                 stratified=True, num_boost_round=1000, \n",
    "#                 early_stopping_rounds=50, verbose_eval=False,\n",
    "#                 as_pandas=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to search over\n",
    "param_grid = {'max_depth'= [3, 5, 7, 10],  # Control complexity\n",
    "              'min_child_weight'= , # The higher, the more regularization\n",
    "              'gamma'= 0, # Higher value leads to fewer splits for a given node (i.e. more regularization) if\n",
    "              'subsample'= [0.5, 0.75, 1],  # Fraction of observations per tree \n",
    "              'colsample_bytree': [0.5, 0.75, 1]} # Fraction of features per tree\n",
    "# Grid search\n",
    "xgb_gs_1 = GridSearchCV(xgb_1, param_grid=param_grid, \n",
    "                        num_boo\n",
    "                        scoring='average_precision',\n",
    "                        return_train_score=True,\n",
    "                        n_jobs=n_jobs, cv=3, verbose=5)\n",
    "xgb_gs_1.fit(X_train_small, y_train_small) \n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_gs_1, 'saved_models/xgb_gs_1.joblib')\n",
    "\n",
    "# Predictions\n",
    "y_proba_xgb = xgb_.predict_proba(X_test_small)[:, 1]\n",
    "average_precision_score(y_test_small, y_proba_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_1 = XGBClassifier(objective='binary:logistic', learning_rate=0.1,\n",
    "               n_estimators=1000,\n",
    "               max_depth=5,min_child_weight=1, gamma=0,\n",
    "               subsample=0.8, colsample_bytree=0.8,\n",
    "               seed=0, nthread=n_jobs)\n",
    "xgb_1.fit(X_train_small, y_train_small,\n",
    "          early_stopping_rounds=50,\n",
    "          eval_set=[(X_test_small, y_test_small)],\n",
    "          eval_metric='map'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# joblib.dump(xgb_gs_1, 'saved_models/xgb_gs_1.joblib')\n",
    "\n",
    "# Predictions\n",
    "y_proba_xgb = xgb_1.predict_proba(X_test_small)[:, 1]\n",
    "average_precision_score(y_test_small, y_proba_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':'binary:logistic', 'eval_metric':'map',\n",
    "         'scale_pos_weight':5,  # Balance class weight\n",
    "         'seed':0}\n",
    "xgb_cv = xgb.cv(dtrain=data_xgb_train, params=params, nfold=3,\n",
    "                num_boost_round=50, as_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb_cv = xgb_cv.predict(X_test_small)\n",
    "average_precision_score(y_test_small, y_xgb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpsklearn\n",
    "dir(hpsklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf_1 = SVC(kernel='rbf', probability=False, \n",
    "              cache_size=5000, class_weight='balanced')\n",
    "# Parameters to search over\n",
    "param_grid = {'C': np.logspace(-3, 3, 8),\n",
    "              'gamma': np.logspace(-5, 1, 8)}\n",
    "# Define indices for validation split (instead of proper cross-validation)\n",
    "split_indices = ShuffleSplit(n_splits=1, test_size=.2, random_state=1)\n",
    "\n",
    "svm_rbf_gs_1 = GridSearchCV(svm_rbf_1, param_grid=param_grid,\n",
    "                          return_train_score=True,\n",
    "                          scoring='average_precision', cv=split_indices)\n",
    "svm_rbf_gs_1.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(svm_rbf_gs_1, 'saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "svm_rbf_gs_1 = joblib.load('saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of class\n",
    "y_pred_svm_rbf_1 = svm_rbf_gs_1.predict(X_test_small)\n",
    "\n",
    "# Distance from separating hyperplane\n",
    "y_pred_distance_svm_rbf_1 = svm_rbf_gs_1.decision_function(X_test_small)\n",
    "\n",
    "# Save results\n",
    "average_precision_1['SVM (RBF Kernel)'] = \\\n",
    "    average_precision_score(y_test_small, y_pred_distance_svm_rbf_1)\n",
    "\n",
    "classification_reports_1['SVM (RBF Kernel)'] = \\\n",
    "    classification_report(y_test_small, y_pred_svm_rbf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_best_result(svm_rbf_gs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_heatmap(svm_rbf_gs_1) #, x_digits=1, x_scientific_notation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_rbf_gs_1.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
