{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Predictive-models\" data-toc-modified-id=\"Predictive-models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Predictive models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Random Forests</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Support-Vector-Machine\" data-toc-modified-id=\"Support-Vector-Machine-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Support Vector Machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-SVM\" data-toc-modified-id=\"Linear-SVM-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Linear SVM</a></span></li><li><span><a href=\"#SVM-with-RBF-Kernel\" data-toc-modified-id=\"SVM-with-RBF-Kernel-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>SVM with RBF-Kernel</a></span></li><li><span><a href=\"#SVM-with-polynomial-kernel\" data-toc-modified-id=\"SVM-with-polynomial-kernel-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>SVM with polynomial kernel</a></span></li></ul></li></ul></li><li><span><a href=\"#Performance-comparison\" data-toc-modified-id=\"Performance-comparison-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Performance comparison</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "import missingno  # for visualizing missing data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, \\\n",
    "    LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    "    precision_recall_curve, average_precision_score, f1_score, \\\n",
    "    roc_curve, auc, roc_auc_score, make_scorer,\\\n",
    "    accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of CPU cores to use\n",
    "n_jobs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "with open('data_processed/all_data.pickle', 'rb') as pickled_file: \n",
    "    all_data = pickle.load(pickled_file) \n",
    "\n",
    "# TEMPORARY:\n",
    "# ---------\n",
    "# Reduce number of observations to speed up computations\n",
    "n_samples=100000\n",
    "all_data = resample(all_data, replace=False, \n",
    "                    n_samples=n_samples, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data types of columns that were not transformed:\n",
      " [dtype('bool')]\n"
     ]
    }
   ],
   "source": [
    "# Find categorical variables with more than 50 unique values and drop them\n",
    "unique_values_cat = all_data.select_dtypes(include='object') \\\n",
    "                        .nunique() \\\n",
    "                        .sort_values(ascending=False) \n",
    "# Drop categorical variables with more than 50 categories\n",
    "all_data = all_data.drop(unique_values_cat[unique_values_cat > 50].index,\n",
    "                 axis='columns')\n",
    "\n",
    "# Train-test split (for now only select 1% for training)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(all_data.drop('default', axis='columns'),\n",
    "                     all_data.default,\n",
    "#                      train_size=0.9,\n",
    "                     train_size=20000, test_size=1000,\n",
    "                     random_state=1,\n",
    "                     shuffle=True, stratify=all_data.default) \n",
    "\n",
    "# Imputation and standardization for numeric features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_transformer = Pipeline(steps =[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())]) \n",
    "\n",
    "# Imputation and one-hot encoding for categorical features\n",
    "categorical_features = X_train.select_dtypes(include=[object]).columns\n",
    "categorical_transformer = Pipeline(steps =[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combining preprocessing for both kinds of features\n",
    "# (Features of other dtypes – in our case, boolean – will be\n",
    "# appended at the end without transformation.)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_transformer', \n",
    "             numeric_transformer, numeric_features),\n",
    "        ('categorical_transformer', \n",
    "             categorical_transformer, categorical_features)],\n",
    "    remainder='passthrough', n_jobs=n_jobs)\n",
    "\n",
    "# Print dtypes of untransformed data\n",
    "print('data types of columns that were not transformed:\\n {}'\n",
    "        .format(X_train.select_dtypes(exclude=[np.number, object]) \\\n",
    "                .dtypes.unique()))\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_p = preprocessor.fit_transform(X_train)\n",
    "X_test_p = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to store results\n",
    "average_precision = {}\n",
    "classification_reports = {}\n",
    "most_important_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "        oob_score=False, n_jobs=n_jobs, random_state=1,\n",
    "        class_weight='balanced_subsample')\n",
    "rf.fit(X_train_p, y_train)\n",
    "\n",
    "# Predictions of class and probability\n",
    "y_pred_rf = rf.predict(X_test_p) \n",
    "y_pred_proba_rf = rf.predict_proba(X_test_p)[:, 1]\n",
    "\n",
    "# Save results\n",
    "average_precision['random forests'] = \\\n",
    "    average_precision_score(y_test, y_pred_proba_rf)\n",
    "classification_reports['random forests'] = \\\n",
    "    classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Save most important features\n",
    "# First get a list of feature names for each dtype\n",
    "categorical_names = preprocessor \\\n",
    "    .named_transformers_['categorical_transformer'] \\\n",
    "    .named_steps['onehot'] \\\n",
    "    .get_feature_names()\n",
    "other_names = X_train \\\n",
    "    .select_dtypes(exclude=[np.number, object]) \\\n",
    "    .columns\n",
    "# Concatenate feature names\n",
    "feature_names = \\\n",
    "    list(numeric_features) + list(categorical_names) + \\\n",
    "        list(other_names) \n",
    "# Compute feature importance and sort\n",
    "most_important_features['random forests'] = \\\n",
    "    pd.Series(rf.feature_importances_, index=feature_names) \\\n",
    "            .sort_values(ascending=False) \\\n",
    "            .iloc[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression (Elastic Net) \n",
    "# --------------------------------\n",
    "lr = SGDClassifier(loss='log', penalty='elasticnet', \n",
    "                      class_weight='balanced', \n",
    "                      max_iter=1000, tol=1E-3, # those are defaults for sklearn 0.21+\n",
    "                      random_state=1, n_jobs=n_jobs) \n",
    "\n",
    "# Parameters to search over\n",
    "param_grid = {'l1_ratio': np.linspace(0, 1, 3),\n",
    "              'alpha': np.logspace(-10, 1, 3)}\n",
    "# Grid search\n",
    "lr_gs = GridSearchCV(lr, param_grid=param_grid, \n",
    "                     scoring='average_precision',\n",
    "                     return_train_score=True,\n",
    "                     n_jobs=n_jobs, cv=3)\n",
    "lr_gs.fit(X_train_p, y_train) \n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_gs.predict(X_test_p)    \n",
    "y_pred_proba_lr = lr_gs.predict_proba(X_test_p)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "average_precision['logistic regression'] = \\\n",
    "    average_precision_score(y_test, y_pred_proba_lr)\n",
    "classification_reports['logistic regression'] = \\\n",
    "    classification_report(y_test, y_pred_lr)\n",
    "\n",
    "# Compute feature importance and sort\n",
    "most_important_features['logistic regression'] = \\\n",
    "    pd.Series(lr_gs.best_estimator_.coef_[0], \n",
    "          index=feature_names) \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .iloc[: 10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3453702376955112"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score of best model\n",
    "lr_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "# ----------\n",
    "# Elastic net, logistic regression (Loss='hinge')\n",
    "lr_en = SGDClassifier(loss='log', penalty='elasticnet', random_state=1,\n",
    "                   class_weight='balanced', n_jobs=n_jobs) \n",
    "\n",
    "# Parameters to search over\n",
    "grid={'l1_ratio': np.linspace(0, 1, 4),\n",
    "      'alpha': np.logspace(-10, 1, 11)}\n",
    "\n",
    "\n",
    "svm_lin = LinearSVC(penalty='l2', class_weight='balanced', \n",
    "                  dual=False)\n",
    "param_grid = {'C': np.logspace(-3, 3, 5)}\n",
    "svm_lin_gs = GridSearchCV(svm_lin, param_grid=param_grid,\n",
    "                      scoring=make_scorer(average_precision_score),\n",
    "                                       #   needs_proba=True),\n",
    "                     n_jobs=n_jobs, cv=4)\n",
    "svm_lin_gs.fit(X_train_p, y_train)\n",
    "\n",
    "# Prediction of class\n",
    "y_pred_svm = svm_lin_gs.predict(X_test_p)\n",
    "# Distance from separating hyperplane\n",
    "y_pred_distance_svm = svm_lin_gs.decision_function(X_test_p)\n",
    "\n",
    "# Save results\n",
    "average_precision[('SVM', df_name)] = \\\n",
    "    average_precision_score(y_test, y_pred_distance_svm)\n",
    "classification_reports[('SVM', df_name)] = \\\n",
    "    classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with RBF-Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = SVC(kernel='rbf', probability=False, \n",
    "              cache_size=5000, class_weight='balanced')\n",
    "# Parameters to search over\n",
    "param_grid = {'C': np.logspace(-3, 1, 2),\n",
    "              'gamma': np.logspace(-3, 2, 3)}\n",
    "# Define indices (for train-test split instead of proper cross-validation)\n",
    "split_indices = ShuffleSplit(n_splits=1, test_size=.2, random_state=1)\n",
    "\n",
    "svm_rbf_gs = GridSearchCV(svm_rbf, param_grid=param_grid,\n",
    "                          scoring='average_precision', cv=split_indices)\n",
    "svm_rbf_gs.fit(X_train_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of class\n",
    "y_pred_svm = svm_rbf_gs.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance from separating hyperplane\n",
    "y_pred_distance_svm = svm_rbf_gs.decision_function(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "average_precision['SVM (RBF Kernel)'] = \\\n",
    "    average_precision_score(y_test, y_pred_distance_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports['SVM (RBF Kernel)'] = \\\n",
    "    classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3893497755284131"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\t\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\t\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\t\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\t\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'C': 0.001, 'gamma': 0.001}</th>\n",
       "      <td>0.348660</td>\n",
       "      <td>0.354288</td>\n",
       "      <td>0.332209</td>\n",
       "      <td>0.365111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.356518</td>\n",
       "      <td>0.352057</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'C': 0.001, 'gamma': 0.31622776601683794}</th>\n",
       "      <td>0.262435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.260647</td>\n",
       "      <td>0.264222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'C': 0.001, 'gamma': 100.0}</th>\n",
       "      <td>0.204500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'C': 10.0, 'gamma': 0.001}</th>\n",
       "      <td>0.389350</td>\n",
       "      <td>0.437067</td>\n",
       "      <td>0.376807</td>\n",
       "      <td>0.401892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.448363</td>\n",
       "      <td>0.425772</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'C': 10.0, 'gamma': 0.31622776601683794}</th>\n",
       "      <td>0.257872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.257478</td>\n",
       "      <td>0.258266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'C': 10.0, 'gamma': 100.0}</th>\n",
       "      <td>0.204500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>0.204500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mean_test_score  mean_train_score  split0_test_score  split1_test_score  \\\n",
       "params                                                                                                                \n",
       "{'C': 0.001, 'gamma': 0.001}                       0.348660          0.354288           0.332209           0.365111   \n",
       "{'C': 0.001, 'gamma': 0.31622776601683794}         0.262435          1.000000           0.260647           0.264222   \n",
       "{'C': 0.001, 'gamma': 100.0}                       0.204500          1.000000           0.204500           0.204500   \n",
       "{'C': 10.0, 'gamma': 0.001}                        0.389350          0.437067           0.376807           0.401892   \n",
       "{'C': 10.0, 'gamma': 0.31622776601683794}          0.257872          1.000000           0.257478           0.258266   \n",
       "{'C': 10.0, 'gamma': 100.0}                        0.204500          1.000000           0.204500           0.204500   \n",
       "\n",
       "                                            split2_test_score  split0_train_score  split1_train_score  \\\n",
       "params                                                                                                  \n",
       "{'C': 0.001, 'gamma': 0.001}                              NaN            0.356518            0.352057   \n",
       "{'C': 0.001, 'gamma': 0.31622776601683794}                NaN            0.999999            1.000000   \n",
       "{'C': 0.001, 'gamma': 100.0}                              NaN            1.000000            1.000000   \n",
       "{'C': 10.0, 'gamma': 0.001}                               NaN            0.448363            0.425772   \n",
       "{'C': 10.0, 'gamma': 0.31622776601683794}                 NaN            1.000000            1.000000   \n",
       "{'C': 10.0, 'gamma': 100.0}                               NaN            1.000000            1.000000   \n",
       "\n",
       "                                            split2_train_score  \n",
       "params                                                          \n",
       "{'C': 0.001, 'gamma': 0.001}                               NaN  \n",
       "{'C': 0.001, 'gamma': 0.31622776601683794}                 NaN  \n",
       "{'C': 0.001, 'gamma': 100.0}                               NaN  \n",
       "{'C': 10.0, 'gamma': 0.001}                                NaN  \n",
       "{'C': 10.0, 'gamma': 0.31622776601683794}                  NaN  \n",
       "{'C': 10.0, 'gamma': 100.0}                                NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svm_rbf_gs.cv_results_) \\\n",
    "    .set_index('params') \\\n",
    "    .loc[:,['mean_test_score', 'mean_train_score',\n",
    "           'split0_test_score', 'split1_test_score','split2_test_score',\n",
    "           'split0_train_score', 'split1_train_score','split2_train_score']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with polynomial kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random forests': 0.3567486213824882,\n",
       " 'SVM (RBF Kernel)': 0.3904120052613318,\n",
       " 'logistic regression': 0.3486049688125562}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = \n",
    "plt.title('Impact of Dropping Columns on Classifier Performance')\n",
    "plt.ylabel('Average Precision');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.724px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
