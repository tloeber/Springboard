{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carry-out-operations-common-to-all-versions-of-data\" data-toc-modified-id=\"Carry-out-operations-common-to-all-versions-of-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carry out operations common to all versions of data</a></span></li><li><span><a href=\"#Preprocessing-and-predictive-modeling\" data-toc-modified-id=\"Preprocessing-and-predictive-modeling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing and predictive modeling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests how the data cleaning and feature engineering steps implemented in the previous notebook affect predictive accuracy.\n",
    "\n",
    "To do this, I start by creating one large function that can carry out all those steps, but contains switches to allow us to only perform a subset of these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "import missingno  # for visualizing missing data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, \\\n",
    "    LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    "    precision_recall_curve, average_precision_score, f1_score, \\\n",
    "    roc_curve, auc, roc_auc_score, make_scorer\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carry out operations common to all versions of data\n",
    "Before actually defining this large function, we will perform a few essential steps we want to perform or all versions of the data. This will speed up our computations by avoiding unnecessary repetitions.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing those steps comment all the data, we will save these partially cleaned data under a different name. For each version of the data we will create later, we can use this data frame is a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry out operations common to all versions of data\n",
    "# Get all the csv files in the data directory\n",
    "files = glob.glob('data_raw/*.csv')\n",
    "\n",
    "# Read those files into pandas\n",
    "all_data_list = [pd.read_csv(file, header=1, low_memory=False) for file in files]\n",
    "\n",
    "# Concatenate them into one DataFrame\n",
    "all_data = pd.concat(all_data_list, ignore_index=True)\n",
    "# Get rid of the individual DataFrames\n",
    "del all_data_list \n",
    "\n",
    "\n",
    "# TEMPORARY:\n",
    "# ---------\n",
    "# Reduce number of observations next meet up computations\n",
    "n_samples=100000\n",
    "all_data = resample(all_data, replace=False, \n",
    "                    n_samples=n_samples, random_state=1)\n",
    "\n",
    "\n",
    "# Create target variable\n",
    "all_data['default'] = np.NaN\n",
    "all_data.loc[all_data.loan_status.str.contains('Fully Paid', na=False), 'default'] = 0 \n",
    "all_data.loc[(all_data.loan_status.str.contains('Charged Off', na=False)|\n",
    "             (all_data.loan_status=='Default')), 'default'] = 1\n",
    "\n",
    "# Set an impossible value to missing\n",
    "all_data.loc[all_data.dti==-1, 'dti'] = np.nan\n",
    "\n",
    "# Drop loans with indeterminate status\n",
    "all_data = all_data.loc[all_data.default.notnull(),:]\n",
    "\n",
    "# Find columns with 0 or 1 unique values\n",
    "constant_or_missing = [col for col in all_data.columns \n",
    "                           if all_data.loc[:,col] \\\n",
    "                                   .value_counts() \\\n",
    "                                   .shape[0] \\\n",
    "                                <= 1] \n",
    "\n",
    "# Delete those columns\n",
    "all_data = all_data.drop(constant_or_missing + ['loan_status'], axis=1)\n",
    "\n",
    "# Create hierarchical INDEX with date and ID,\n",
    "# then save this version of the date as a basis for further transformations\n",
    "all_data['issue_d'] = pd.to_datetime(all_data.loc[:,'issue_d'], format='%b-%Y')\n",
    "all_data_partially_cleaned = all_data.set_index(['issue_d', 'id']).sort_index()\n",
    "\n",
    "\n",
    "# Define endogenous and irrelevant columns, another columns to potentially drop\n",
    "irrelevant = ['url', 'initial_list_status']\n",
    "endogenous = ['out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', \n",
    "              'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', \n",
    "              'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', \n",
    "              'debt_settlement_flag', 'last_fico_range_low', 'last_fico_range_high',\n",
    "              'last_credit_pull_d']\n",
    "lc = ['int_rate', 'grade', 'sub_grade']\n",
    "other_to_drop = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv']\n",
    "\n",
    "\n",
    "# Define function that creates different versions of data \n",
    "def make_data(df, v_to_drop=endogenous+lc+irrelevant+other_to_drop,\n",
    "              drop_missing ='ideal', transform_time_since=True,\n",
    "              transform_types=True, normalize_by_income=True,\n",
    "              transform_skewed_v=True):\n",
    "    \"\"\"\n",
    "    Creates different versions of data to compare performance of predictive\n",
    "    models on data with different amount of cleaning, data engineering, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the partially cleaned version of the data\n",
    "    df = all_data_partially_cleaned\n",
    "        \n",
    "    # Drop ENDOGENOUS and IRRELEVANT variables\n",
    "    # ------------------------------------------\n",
    "    # Drop variables\n",
    "    df = df.drop(v_to_drop, axis='columns')\n",
    "    # Inform user about which variables were dropped\n",
    "    print('\\nEndogenous and irrelevant variables dropped: {}'\n",
    "              .format(v_to_drop))\n",
    "\n",
    "    \n",
    "    ## MISSING VALUES\n",
    "    # ---------------\n",
    "    # Calculate the proportion of missing values for each variable\n",
    "    missing = (df.isnull().sum()/df.shape[0]) \\\n",
    "                    .sort_values(ascending=False)\n",
    "    variables_to_inspect = ['mths_since_last_record', 'mths_since_recent_bc_dlq',\n",
    "                            'mths_since_last_major_derog', 'mths_since_recent_revol_delinq',\n",
    "                            'mths_since_last_delinq']\n",
    "    # If specified, drop missing values as we found to be optimal in the data cleaning notebook \n",
    "    if drop_missing == 'ideal':\n",
    "        variables_to_plot_1 = missing[missing>.75].index # Select columns\n",
    "        # Create a list of variables to delete, and add all the plotted variables except the last two\n",
    "        variables_to_drop = list(variables_to_plot_1[:-2])\n",
    "        variables_to_plot_2 = missing[(missing <= .75) & (missing > .3)].index # Select columns   \n",
    "        # Add variables to list of variables to delete\n",
    "        variables_to_drop.extend([v for v in variables_to_plot_2 if v not in variables_to_inspect])\n",
    "        variables_to_plot_3 = missing[(missing <= .3) & (missing > .005)].index # Select columns\n",
    "        # Drop variables with too many missing values\n",
    "        df = df.drop(variables_to_drop, axis='columns')\n",
    "        # Inform user\n",
    "        print('Dropping variables with missing values as determined to be optimal in '\n",
    "              'the data cleaning notebook')\n",
    "    \n",
    "    # If the user-specified specified do not drop any variables, continue\n",
    "    \n",
    "    elif (drop_missing is None): \n",
    "        print('No variables were dropped due to missing values')\n",
    "    \n",
    "    # If specified, drop columns based on proportion of missing values\n",
    "    else:\n",
    "        # Get variables whose proportion of missing values exceeds threshold\n",
    "        variables_to_drop = missing.loc[missing > drop_missing].index\n",
    "        # Drop them\n",
    "        df = df.drop(variables_to_drop, axis='columns')\n",
    "        # Inform user about which variables were dropped\n",
    "        print('Dropping variables with more than {}% missing values:\\n{}' \n",
    "                .format(drop_missing * 100, variables_to_drop))\n",
    "\n",
    "\n",
    "    # Transform \"TIME-SINCE\" variables\n",
    "    # -------------------------------\n",
    "    if transform_time_since: \n",
    "        # List to store transformed variables\n",
    "        new_variables = []\n",
    "        # Create a list with corresponding counts for each variables\n",
    "        event_counts = ['pub_rec', None, None, None, 'delinq_2yrs']\n",
    "\n",
    "        # Perform transformations\n",
    "        for v, count in zip(variables_to_inspect, event_counts):\n",
    "            # Get the variable and add 1\n",
    "            v_transf = df.loc[:, v] + 1\n",
    "            # Perform power transformation\n",
    "            v_transf = v_transf ** -0.5\n",
    "            # pdb.set_trace()\n",
    "            # Set values to zero where event did not occur\n",
    "            if count is not None: # If we have a count variable:\n",
    "                # Get boolean mask where event did not occur\n",
    "                zero_count = df.loc[:, count]==0\n",
    "                # Set values to zero \n",
    "                v_transf.loc[zero_count] = 0\n",
    "            else: # Where we don't hâ€“ave event count, set all missings to zero\n",
    "                v_transf = v_transf.fillna(0)\n",
    "\n",
    "            # Store transformed variable\n",
    "            new_variables.append(v_transf)\n",
    "\n",
    "        # Concatenate transformd variables into data frame\n",
    "        new_df = pd.concat(new_variables, axis='columns')\n",
    "        ['inv_sqrt_1p_' + v for v in variables_to_inspect]\n",
    "        # Add variable names\n",
    "        new_df.columns = ['inv_sqrt_1p_' + v for v in variables_to_inspect]\n",
    "        # Print proportion of non-missing observations\n",
    "        # Add variables to our main data frame\n",
    "        df = pd.concat([df, new_df], axis='columns')\n",
    "        # Delete temporary data frame\n",
    "        del new_df\n",
    "\n",
    "        # Drop original variables\n",
    "        df = df.drop(variables_to_inspect, axis='columns')\n",
    "        \n",
    "        # Inform user\n",
    "        print('Transformed \"time-since\" variable')\n",
    "        \n",
    "\n",
    "    \n",
    "    # Making sure variables are of the right TYPE\n",
    "    # -------------------------------------------\n",
    "    if transform_types:\n",
    "        integers = [variable for variable in df.dtypes[df.dtypes==float].index\n",
    "                        if df.loc[:, variable].apply(float.is_integer).all()]\n",
    "        df[integers] = df.loc[:, integers].astype(int)\n",
    "\n",
    "        # Convert objects to strings\n",
    "        for variable in ['revol_util']:\n",
    "            df[variable] = pd.to_numeric(df.loc[:, variable].str.replace('%', ''), errors='coerce')\n",
    "\n",
    "        df['term_5y'] = np.nan\n",
    "        df.loc[df.term.str.contains('60 months'), 'term_5y'] = 1\n",
    "        df.loc[df.term.str.contains('36 months'), 'term_5y'] = 0\n",
    "\n",
    "\n",
    "        df['earliest_cr_line'] = pd.to_datetime(df.loc[:, 'earliest_cr_line'], format='%b-%Y')\n",
    "\n",
    "        # Create new variables  with time difference\n",
    "        df['earliest_cr_line_days'] = \\\n",
    "            (df.reset_index(level='issue_d').issue_d.values\n",
    "            - df.earliest_cr_line).dt.days\n",
    "\n",
    "        # Mark original variables for deletion\n",
    "        variables_to_drop_3 = 'earliest_cr_line'\n",
    "        # Delete Variables\n",
    "        df = df.drop(variables_to_drop_3, axis='columns')\n",
    "        # Inform user\n",
    "        print('Transformed types')\n",
    "    \n",
    "\n",
    "    # NORMALIZE monetary quantities BY INCOME\n",
    "    # ---------------------------------------\n",
    "    if normalize_by_income:\n",
    "        df.assign(\n",
    "            revol_bal_to_inc = \n",
    "                df.revol_bal / df.annual_inc,\n",
    "            installment_to_inc = \n",
    "                (12*df.installment) / df.annual_inc,\n",
    "            tot_coll_amt_to_inc = \n",
    "                df.tot_coll_amt / df.annual_inc,\n",
    "            tot_cur_bal_to_inc = \n",
    "                df.tot_cur_bal / df.annual_inc,\n",
    "            total_bal_ex_mort_to_inc = \n",
    "                df.total_bal_ex_mort / df.annual_inc)\n",
    "\n",
    "        # Delete original variables, where applicable\n",
    "        variables_to_drop_2 = \\\n",
    "            ['revol_bal', 'installment', \n",
    "             'tot_cur_bal', 'total_bal_ex_mort']\n",
    "        df = df.drop(variables_to_drop_2, axis='columns')\n",
    "        # Inform user\n",
    "        print('Normalized monetary quantities by income')\n",
    "        \n",
    "\n",
    "    # If specified, transform SKEWED variables\n",
    "    # ----------------------------------------\n",
    "\n",
    "    if transform_skewed_v:\n",
    "        # Inform the user\n",
    "        print('Transformed overly-skewed variables')\n",
    "        def mode_at_extremum(df):\n",
    "            \"\"\"\n",
    "            Looks for variables where the mode occurs at the minimum or maximum.\n",
    "\n",
    "            This is of interest because of censoring: Many variables have a natural\n",
    "            minimum or maximum, e.g. 0 or 100%, respectively. \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df: pd.DataFrame\n",
    "                Data frame whose variables to check .\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            tuple containing two lists\n",
    "                First list contains names of variables whose mode occurs at the minimum.\n",
    "                Second list contains names of variables whose mode occurs at the maximum.\n",
    "            \"\"\"\n",
    "\n",
    "            # Get mode(s) for each variable in the data frame\n",
    "            modes = df.mode()\n",
    "\n",
    "            # Get minimum and maximum for each variable in the data frame\n",
    "            minimum = pd.DataFrame(df.min())\n",
    "            maximum = pd.DataFrame(df.max())\n",
    "\n",
    "            # Loop over variables and save names of variables were mode occurs at an\n",
    "            # extremum.\n",
    "            mode_at_min = []  \n",
    "            mode_at_max = []\n",
    "            for variable in df:\n",
    "                # Drop missing values from mode and try converting it to scalar\n",
    "                try:\n",
    "                    mode_ = np.asscalar(modes.loc[modes[variable].notnull(), variable])\n",
    "                # If a variable contains multiple modes, we are not interested in it\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    # Convert minimum and maximum to scalars\n",
    "                    min_ = np.asscalar(minimum.loc[variable])\n",
    "                    max_ = np.asscalar(maximum.loc[variable])\n",
    "\n",
    "                    # Test whether minimum or maximum occurs at the mode\n",
    "                    if (mode_ == min_):\n",
    "                        mode_at_min.append(variable)\n",
    "                    elif (mode_ == max_):\n",
    "                        mode_at_max.append(variable)\n",
    "\n",
    "            return(mode_at_min, mode_at_max)\n",
    "\n",
    "        numerics = all_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        mode_at_min, mode_at_max = mode_at_extremum(all_data.loc[:, numerics])\n",
    "\n",
    "        \n",
    "\n",
    "        def find_mixtures(df, variables, n_unique_threshold=10,\n",
    "                          proportion_threshold=0.01, ratio_threshold=5, \n",
    "                          verbose=False):\n",
    "            \"\"\"\n",
    "            Identify variables for which we should include a dummy for overly frequent unique values.\n",
    "\n",
    "            For each variable in a DataFrame, identify unique values that occur so often that\n",
    "            this variable is best modeled as being generated by a mixture process. I \n",
    "            operationalize such a value as occurring at least 10 times as often as would be \n",
    "            expected if all unique values occurred with the same frequency. For example, if\n",
    "            the variable has 15 unique values, \n",
    "\n",
    "            A mixture process looks as follows:\n",
    "            First, a Bernoulli variable determines if the variable takes on a particular discrete \n",
    "            value. This may be repeated for other particular discrete values.  If this is not \n",
    "            the case, the value is drawn from a different process such as a normal or poisson\n",
    "            distribution. \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df: pd.DataFrame\n",
    "                Data frame containing the variables to analyze.\n",
    "            variables: list\n",
    "                Variables, located in df, which to check. Usually these will be variables that\n",
    "                have their mode at their minimum or maximum (e.g., the output from\n",
    "                mode_at_extremum() ).\n",
    "            n_unique_threshold: int\n",
    "                The minimum number of unique values of variable must have in order to be \n",
    "                considered.\n",
    "            proportion_threshold: numeric, optional\n",
    "                The minimum proportion of observations in the samples that must have the most\n",
    "                frequent value in order for the variable to be considered. This can be used to\n",
    "                filter out variables where no value occurs frequently. Default: 1%.\n",
    "            ratio_threshold: numeric, optional\n",
    "                Threshold to use for the ratio of the most frequent to the second-most frequent \n",
    "                count. If it is exceeded, the variable will be treated as a deriving from a \n",
    "                mixture distribution. Default: 5\n",
    "            verbose: bool\n",
    "                If true, print out the ratio of the most frequent to the second-most frequent\n",
    "                count for each variable. Default: False\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            list\n",
    "                Names of variables that seem to derive from a mixture distribution.\n",
    "            \"\"\"\n",
    "\n",
    "            mixtures = []\n",
    "            # Go through all variables and look for mixtures\n",
    "            for variable in variables:\n",
    "                # Count unique values\n",
    "                valuecounts = df.loc[:, variable].value_counts()\n",
    "\n",
    "                # Skip variables whose number of unique values does not exceed the\n",
    "                # supplied threshold\n",
    "                if len(valuecounts) < n_unique_threshold:\n",
    "                    continue\n",
    "\n",
    "                # Skip variables where no single value occurs frequently \n",
    "                if valuecounts.iloc[0] < proportion_threshold * len(df):\n",
    "                    continue\n",
    "\n",
    "                # Compute ratio of occurrence of most-frequent to second-most frequent value\n",
    "                ratio = valuecounts.iloc[0] / valuecounts.iloc[1]\n",
    "\n",
    "                # If ratio is greater than the threshold, save variable name\n",
    "                if ratio > ratio_threshold:\n",
    "                    mixtures.append(variable)\n",
    "\n",
    "            return(mixtures)\n",
    "\n",
    "        def add_mixture_dummies(df, mixture_min, mixture_max):\n",
    "            # Initialize dictionary to store dummy variables\n",
    "            mixture_dummies = {}\n",
    "            # Create dummies for variables with mode at maximum\n",
    "            for variable in mixture_max:\n",
    "                dummy_name = 'max_' + variable\n",
    "                mixture_dummies[dummy_name] = df.loc[:, variable] == \\\n",
    "                                              df.loc[:, variable].max()\n",
    "\n",
    "            # Create dummies for variables with mode at minimum\n",
    "            for variable in mixture_min:\n",
    "                dummy_name = 'min_' + variable\n",
    "                mixture_dummies[dummy_name] = df.loc[:, variable] == \\\n",
    "                                              df.loc[:, variable].min()\n",
    "\n",
    "            # Create data frame with all dummies\n",
    "            mixture_dummies = pd.DataFrame(mixture_dummies)\n",
    "            # pdb.set_trace ()\n",
    "            # Add dummies to original data\n",
    "            df = pd.concat([df, mixture_dummies], axis='columns')\n",
    "\n",
    "            return(df)\n",
    "\n",
    "        def find_mixtures_add_dummies(df, dry_run=True, **kwargs):\n",
    "            \"\"\"\n",
    "            Identify variables for which we should include a dummy for overly frequent \n",
    "            unique values. Print variable names or go ahead and add dummies.\n",
    "\n",
    "            Identify numeric variables where we should add a dummy variable for the mode to\n",
    "            model the extreme skewness that results from mixture processes such as\n",
    "            zero-inflation. \n",
    "\n",
    "            For each numeric variable in a DataFrame, identify unique values that are \n",
    "            located at the minimum or maximum, and that occur so often that this variable is\n",
    "            best modeled as being generated by a mixture process. The decision rule is\n",
    "            based on the ratio of the most frequent count to the second-most frequent\n",
    "            count. By default, this threshold is set to 5.\n",
    "\n",
    "            The subset of mixture processes we are interested in here look as follows:\n",
    "            First, a raw from a Bernoulli distribution decides if the variable takes on\n",
    "            a particular discrete value (often 0 for counts or 100% for proportions). \n",
    "            If this is not the case, the variable's value is drawn from a different\n",
    "            distribution such as a normal or poisson distribution. For simplicity, \n",
    "            this subtype of mixture process will simply be referred to as a mixture \n",
    "            process, since there is no more exact term.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df: pd.DataFrame\n",
    "                Data frame to inspect and modify.\n",
    "            dry_run: bool, optional\n",
    "                Whether to simply print for which variables would be added, or whether\n",
    "                to actually add those dummies to the input data frame. Default: only\n",
    "                print variable names.\n",
    "            **kwargs\n",
    "                Additional arguments to change the defaults of find_mixture():\n",
    "                n_unique_threshold, ratio_threshold, proportion_threshold, verbose.\n",
    "            \"\"\"\n",
    "\n",
    "            # Consider only variables that are numeric\n",
    "            numerics = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "            # First find variables where mode occurs at minimum or maximum\n",
    "            mode_at_min, mode_at_max = \\\n",
    "                mode_at_extremum(df.loc[:, numerics])\n",
    "\n",
    "            # Find which of those variables have an inflated number of \n",
    "            # minimum or maximum values\n",
    "            mixtures_at_min = find_mixtures(df, mode_at_min, **kwargs)\n",
    "            mixtures_at_max = find_mixtures(df, mode_at_max, **kwargs)\n",
    "\n",
    "            # If specified, add dummies to original data and also return the\n",
    "            # names of variables for which a dummy was added\n",
    "            if dry_run != True: \n",
    "                return(add_mixture_dummies(df, mixtures_at_min, mixtures_at_max),\n",
    "                       mixtures_at_min, mixtures_at_max)\n",
    "\n",
    "\n",
    "        # Print names of variables for which dummies will be added\n",
    "        find_mixtures_add_dummies(df, proportion_threshold=0.001) \n",
    "\n",
    "        # Add dummies and save lists of variables for which we created dummies\n",
    "        df, min_dummies, max_dummies = find_mixtures_add_dummies(df, dry_run=False, proportion_threshold=0.001)\n",
    "        df.columns\n",
    "\n",
    "        def skewness(data, ignore_min=[], ignore_max=[], \n",
    "                     verbose=True, n_threshold=5):\n",
    "            \"\"\"\n",
    "            Returns the skewness of each variable in a Series or DataFrame.\n",
    "\n",
    "            Skewness is operationalized as follows: \n",
    "            log((Upper Quartile - Median)/(Median - Lower Quartile)\n",
    "            If the median falls on the same value as either the upper or lower quartile, the \n",
    "            resulting zero is replaced by a very small number in order to avoid taking the \n",
    "            log of zero or dividing by zero, respectively. If all three quartiles fall on \n",
    "            the same value, no skewness score is produced because our measure is not\n",
    "            appropriate for such extremely skewed cases. \n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : Series or DataFrame\n",
    "                Variable to transform.\n",
    "            n_threshold : int\n",
    "                Minimum number of unique values to compute skewness\n",
    "            verbose : bool\n",
    "                Whether to inform users for which variables the minimum or maximum was \n",
    "                discarded, and for which variables the skewness score is likely unreliable\n",
    "                due to a low number of unique values.  This should only be turned off if\n",
    "                those problematic variables have been removed beforehand. Default: True\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "\n",
    "\n",
    "            If input is of type pd.Series:\n",
    "                float\n",
    "                    skewness\n",
    "            \"\"\"\n",
    "\n",
    "            # Make a copy of the input data to operate on\n",
    "            x = copy.copy(data)\n",
    "\n",
    "            # Compute number of unique values for each variable\n",
    "            if verbose == True:\n",
    "                n_unique = x.nunique()\n",
    "\n",
    "            # Start by trying to treat the input as a SERIES:\n",
    "            try:\n",
    "                # First check whether we need to remove the mode. If so, set to missing\n",
    "                if x.name in ignore_min:\n",
    "                    x.loc[x==x.min()] = np.nan\n",
    "                elif x.name in ignore_max:\n",
    "                    x.loc[x==x.max()] = np.nan\n",
    "\n",
    "                # Compute quartiles \n",
    "                quartiles = x.quantile(q=[.25, .5, .75])          \n",
    "                upper_width = quartiles[.75] - quartiles[.5]\n",
    "                lower_width = quartiles[.5] - quartiles[.25]\n",
    "\n",
    "                # If the three quartiles do NOT fall on the same value, calculate skewness\n",
    "                if (upper_width != 0) or (lower_width != 0):\n",
    "                    # If only the lower and middle quartile fall onto the same value, set \n",
    "                    # their difference to a very small number to avoid introducing missing \n",
    "                    # values due to division by zero.\n",
    "                    if lower_width == 0:\n",
    "                        lower_width = .000001\n",
    "                        # Do the same for the difference between upper quartile and median, \n",
    "                        # since a zero here would prevent us from taking the log later.\n",
    "                    elif upper_width == 0:\n",
    "                        upper_width = .000001\n",
    "                    else: # Is there a better way of putting this?\n",
    "                        pass\n",
    "\n",
    "                    # Calculate skewness score\n",
    "                    skewness = np.log(upper_width / lower_width)\n",
    "                    # If skewness cannot be computed (for reasons other than all three \n",
    "                    # variables falling onto the same value, which is addressed below),\n",
    "                    # a variable. \n",
    "                    if not np.isnan(skewness):\n",
    "                        v_problem = [] # Make it a list for consistency with DataFrames\n",
    "                    else:\n",
    "                        v_problem = [x.name]  \n",
    "\n",
    "                # If all three quartiles fall onto the same value, set skewness to missing\n",
    "                else:\n",
    "                    skewness = np.nan        \n",
    "                    v_problem = []\n",
    "\n",
    "\n",
    "            # If treating the input data as Series doesn't work, treat it as DATAFRAME\n",
    "            except AttributeError:   # Because DataFrame doesn't have name attribute\n",
    "                # Remove min or max from variables where we included a dummy for it\n",
    "                for variable in x:\n",
    "                    if variable in ignore_min:\n",
    "                        x.loc[x[variable] == x[variable].min(), variable] = np.nan\n",
    "                    elif variable in ignore_max:\n",
    "                        x.loc[x[variable] == x[variable].max(), variable] = np.nan\n",
    "\n",
    "\n",
    "                # The following code applies to both SERIES and DATAFRAMEs again:\n",
    "                # Compute quartiles and distance between them\n",
    "                quartiles = x.quantile(q=[.25, .5, .75])          \n",
    "                upper_width = quartiles.apply(lambda col: col[.75] - col[.5])\n",
    "                lower_width = quartiles.apply(lambda col: col[.5] - col[.25])\n",
    "\n",
    "                # Flag location of variables where all three quartiles fall onto the same value, \n",
    "                # so that we can set skewness to missing later (because our measure cannot \n",
    "                # deal with such extreme cases.\n",
    "                extreme_skew = (upper_width == 0) & (lower_width ==0)    \n",
    "                v_no_skewness = x.columns[extreme_skew].tolist()\n",
    "\n",
    "                # If the difference between lower quartile and median is zero, set it to a very small\n",
    "                # number to avoid introducing missing values due to division by zero.\n",
    "                lower_width[lower_width == 0] = .000001\n",
    "                # Do the same for the difference between upper quartile and median, since a zero\n",
    "                # here would prevent us from taking the log later\n",
    "                upper_width[upper_width == 0] = .000001\n",
    "\n",
    "                # Calculate skewness score\n",
    "                skewness = np.log(upper_width / lower_width)\n",
    "\n",
    "                # Identify any missing values for skewness (Note that this is BEFORE\n",
    "                # we manually added missing values for variables where all three quartiles \n",
    "                # fall onto the same value)\n",
    "                if np.isnan(skewness).any():\n",
    "                    v_problem = skewness[skewness.isnull()].index.tolist()\n",
    "\n",
    "                else:\n",
    "                    v_problem = []  \n",
    "\n",
    "                # Set skewness for variables where all three quartiles fall onto \n",
    "                # the same values to missing\n",
    "                skewness.loc[extreme_skew] = np.nan\n",
    "\n",
    "            return(skewness, v_problem)\n",
    "\n",
    "        def log_or_log1p(s):\n",
    "            \"\"\" Returns the log of all strictly positive variables and the log(1+x) \n",
    "            of all non-negative variables. Ignores variables with any negative value. \n",
    "\n",
    "            Args: s, pd.Series\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Raise error for log(0) rather than producing NaNs\n",
    "            with np.errstate(all='raise'):\n",
    "                # Try taking log\n",
    "                try:\n",
    "                    # Exclude missing values manually\n",
    "                    log = np.log(s[s.notnull()])\n",
    "                    log.name = 'log_' + s.name\n",
    "\n",
    "                    return(log)\n",
    "\n",
    "                except FloatingPointError:\n",
    "                    pass  # To avoid nesting\n",
    "\n",
    "                # If this fails, try taking log(1+x)\n",
    "                try:\n",
    "                    log1p = np.log1p(s[s.notnull()]) \n",
    "                    log1p.name = 'log1p_' + s.name\n",
    "                    return(log1p)        \n",
    "\n",
    "                # If this still doesn't work, the variable contains \n",
    "                # negative values, so return None.\n",
    "                except FloatingPointError:\n",
    "                    return(None)\n",
    "\n",
    "                \n",
    "        def transform_skewness(df, min_dummies, max_dummies,\n",
    "                               n_threshold_ignore=3, n_threshold_warn=10, \n",
    "                               take_log=[], ignore=[], dry_run=True):\n",
    "            \"\"\"Applies log-transformation to each variable in a DataFrame where it \n",
    "            reduces skewness.\n",
    "\n",
    "            Only numeric variables with at least 10 unique values are considered (since\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            df : pd.DataFrame\n",
    "                Data to diagnose or transform\n",
    "            min_dummies : list\n",
    "                Variables for which all minimum values should be ignored when\n",
    "                computing skewness.\n",
    "            max_dummies : list\n",
    "                Variables for which all maximum values should be ignored when\n",
    "                computing skewness.\n",
    "            n_threshold_ignore : int\n",
    "                Specifies the minimum number of unique values a variable must \n",
    "                exceed in order to be considered.\n",
    "            n_threshold_warn : int\n",
    "                Specifies the minimum number of unique values a variable must\n",
    "                exceed so that the user is not warned to examine it manually.\n",
    "            take_log : list, optional\n",
    "                Variables for which you take the log without examining them first.\n",
    "                This is particularly useful after already having performed a \n",
    "                manual diagnosis during the dry-run. Default: Empty list.\n",
    "            ignore : list, optional\n",
    "                Variables to ignore. Default: Empty list.\n",
    "            dry_run : boolean\n",
    "                If False, performs the transformations.  If True, prints which\n",
    "                transformations would be performed, along with some additional \n",
    "                information. Default: False.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            If dry_run == False:\n",
    "                df_new : pd.DataFrame\n",
    "                    Transformed data\n",
    "                df_deleted_v: pd.DataFrame\n",
    "                    Deleted (untransformed) variables\n",
    "\n",
    "            If dry_run == True:\n",
    "                None\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Make sure arguments are of the right type\n",
    "            if not isinstance(take_log, list):\n",
    "                raise TypeError('take_log must be of type list.')\n",
    "            if not isinstance(ignore, list):\n",
    "                raise TypeError('ignore must be of type list.')\n",
    "\n",
    "            ## Select only relevant variables\n",
    "            # Get names of all the numeric features\n",
    "            v_numeric_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            # Disregard variables that user-specified to ignore\n",
    "            v_numeric = [v for v in v_numeric_all if v not in ignore]\n",
    "            # Count the number of unique values for each numeric variable\n",
    "            n_unique = df.loc[:, v_numeric].nunique()\n",
    "            # Ignore variables with very low counts (and save their names)\n",
    "            v_very_low_count = n_unique.loc[n_unique <= n_threshold_ignore]\n",
    "\n",
    "            # Make a list of transform  candidates that excludes variables that are\n",
    "            # not numeric or have very low counts of unique values\n",
    "            transform_candidates = [v for v in v_numeric if v not in v_very_low_count]\n",
    "\n",
    "            # Save variables with moderately low number of unique values, so that\n",
    "            # we can suggest to examine them manually\n",
    "            v_low_count = n_unique.loc[(n_unique <= n_threshold_warn) & \n",
    "                                       (n_unique > n_threshold_ignore)]\n",
    "\n",
    "\n",
    "            ##  Try log transformation\n",
    "            # Calculate skewness before transformation (if we added a dummy for\n",
    "            # min or max, ignore those values and calculate skew for other values only)\n",
    "            skew_level, v_problem_level = skewness(df.loc[:, transform_candidates], \n",
    "                                                  ignore_min=min_dummies, \n",
    "                                                  ignore_max=max_dummies,\n",
    "                                                  n_threshold=n_threshold_ignore,\n",
    "                                                  verbose=False)\n",
    "\n",
    "            # Create list to store transformed variables if they are less skewed\n",
    "            v_to_transform = []\n",
    "            # Create a list of the variables to delete because they were transformed\n",
    "            v_to_delete = []\n",
    "            # Create list to store variables where transformation failed due to \n",
    "            # negative values\n",
    "            v_negative = []\n",
    "            # Create list to store variables where transformation failed for \n",
    "            # other reasons\n",
    "            v_trans_failed = []\n",
    "            # Create a list to store variables where user-specified to take the log\n",
    "            # either way but it was not possible.\n",
    "            v_take_log_problem = []\n",
    "            # Create a list to store variables where a problem occurred computing the\n",
    "            # skewness (other than the three quartiles falling onto the same value)\n",
    "            v_problem_log = []\n",
    "            # Create lists to store variables where skewness couldn't be computed\n",
    "            # because the three quartiles fall onto the same value\n",
    "            v_no_skewness_both= []  # Neither could be computed\n",
    "            v_no_skewness_level = []  # Skewness for level couldn't be computed\n",
    "            v_no_skewness_log = []  # Skewness for log couldn't be computed\n",
    "\n",
    "            # Create lists of variables for which to ignore the minimum or \n",
    "            # maximum for the log-transformed variables\n",
    "            min_dummies_log = [prefix + v for v in min_dummies \n",
    "                               for prefix in ['log_', 'log1p_']]\n",
    "            max_dummies_log = [prefix + v for v in max_dummies\n",
    "                               for prefix in ['log_', 'log1p_']]\n",
    "\n",
    "            # Loop over all the numeric variables, apply transformations, and determine\n",
    "            # what minimizes the skewness\n",
    "            for v in transform_candidates:\n",
    "                col = df.loc[:, v]  # Extract column \n",
    "\n",
    "                # Try taking the log\n",
    "                log = log_or_log1p(col)         \n",
    "\n",
    "                # If the user specified to take the log either way, do the following:\n",
    "                # Save the transformed variable, so we can add it to data\n",
    "                if v in take_log:\n",
    "                    v_to_transform.append(log)\n",
    "                    # Save name of untransformed variable, so we can delete it\n",
    "                    v_to_delete.append(v)\n",
    "                    assert len(v_to_transform)==len(v_to_delete) \n",
    "                    # Go on to the next variable\n",
    "                    continue\n",
    "\n",
    "                # If taking the log was possible, calculate skewness. \n",
    "                # (If we added a dummy for min or max, ignore those valuesa\n",
    "                # and calculates skewness only for the other values)\n",
    "                if log is not None:\n",
    "                    # Create updated lists of variables for which to ignore\n",
    "                    # minimum or maximum\n",
    "                    skew_log, v_problem = skewness(log, verbose=False, \n",
    "                                                   n_threshold=n_threshold_ignore,\n",
    "                                                   ignore_min=min_dummies_log,\n",
    "                                                   ignore_max=max_dummies_log)\n",
    "                    # Add names of variables where a problem occurred to the list\n",
    "                    v_problem_log.extend(v_problem)\n",
    "\n",
    "                # If transformation wasn't possible, check if it's due to any \n",
    "                # negative values. Then save name of offending variable.\n",
    "                else:\n",
    "                    #  Since we're taking log(1+x), the threshold is -1, not 0.              \n",
    "                    if df.loc[:, v].min() <= -1:    \n",
    "                        v_negative.append(v)\n",
    "                    else:\n",
    "                        v_trans_failed.append(v)\n",
    "                    # If the user-specified to take the log either way, note\n",
    "                    # that it wasn't possible\n",
    "                    if v in take_log:\n",
    "                        v_take_log_problem.append(v)\n",
    "                    # Move onto the next variable\n",
    "                    continue\n",
    "\n",
    "                # (Execute this block only if log-transformation was successful)    \n",
    "                # Determine if transformation decreased skew\n",
    "                # Compute absolute value of skewness\n",
    "                abs_skew_level = abs(skew_level[v])\n",
    "                abs_skew_log =abs(skew_log)\n",
    "\n",
    "                # If log-transformation decreases skew do the following steps:\n",
    "                # (Remember that if one scalar is NaN, x<y will return False) \n",
    "                if abs_skew_level >= abs_skew_log:\n",
    "                    # Save the transformed variable, so we can add it to data\n",
    "                    v_to_transform.append(log)\n",
    "                    # Save name of untransformed variable, so we can delete it\n",
    "                    v_to_delete.append(v)\n",
    "                    assert len(v_to_transform)==len(v_to_delete) \n",
    "\n",
    "                # If transformation doesn't decrease skewness, proceed to the next variable\n",
    "                elif abs_skew_level <= abs_skew_log:\n",
    "                    continue\n",
    "\n",
    "                # If skewness can't be calculated, save variable name\n",
    "                elif np.isnan(abs_skew_level) and np.isnan(abs_skew_log):\n",
    "                    v_no_skewness_both.append(v)\n",
    "                elif np.isnan(abs_skew_log):\n",
    "                    v_no_skewness_log.append(v)\n",
    "                # If we only have a skewness score for the log, keep log not level\n",
    "                elif np.isnan(abs_skew_level):\n",
    "                    v_no_skewness_level.append(v)\n",
    "                    # Save the transformed variable, so we can add it to data\n",
    "                    v_to_transform.append(log)\n",
    "                    # Save name of untransformed variable, so we can delete it\n",
    "                    v_to_delete.append(v) \n",
    "\n",
    "\n",
    "            # Print and return results\n",
    "            # ------------------------\n",
    "            output_dict = {} # Dictionary with the results to return\n",
    "\n",
    "            # Inform user about important issues\n",
    "            output_dict['skewness_problem'] = v_problem_level + v_problem_log\n",
    "            output_dict['transformation_failed_other_reasons'] = v_trans_failed\n",
    "\n",
    "            # Inform user about less important issues (print only if specified)\n",
    "            # related to the number of unique observations\n",
    "            output_dict['very_low_unique'] = v_very_low_count\n",
    "            output_dict['low_unique'] = v_low_count\n",
    "\n",
    "            # Issues related to log-transformation\n",
    "            output_dict['below_negative_1'] = v_negative\n",
    "            \n",
    "            # Issues related to skewness\n",
    "            output_dict['all_3_quartiles_equal_both'] = v_no_skewness_both\n",
    "            output_dict['all_3_quartiles_equal_log_only'] = v_no_skewness_log\n",
    "            output_dict['all_3_quartiles_equal_level_only'] = v_no_skewness_level\n",
    "           \n",
    "            # If dry-run is True, only return dictionary with results\n",
    "            if dry_run == True:\n",
    "                return(output_dict)\n",
    "\n",
    "\n",
    "            ## Otherwise, modify original data\n",
    "            else:\n",
    "                # Remove level of variables where log decreased skew, and save them to a \n",
    "                # separate data frame (in case we need them later)\n",
    "                df_deleted_v = df.loc[:, v_to_delete]\n",
    "                df_new = df.drop(v_to_delete, axis='columns')\n",
    "\n",
    "                # Create a new data frame with the logs of the deleted variables\n",
    "                df_logs = pd.concat(v_to_transform, axis='columns')\n",
    "                # Add these transformed variables to our new data frame\n",
    "                df_new = pd.concat([df_new, df_logs], axis='columns')\n",
    "\n",
    "                return(df_new, df_deleted_v, output_dict)\n",
    "\n",
    "        # Ignore variables which we already transformed otherwise above \n",
    "        v_to_ignore = ['inv_sqrt_1p_' + v for v in variables_to_inspect]\n",
    "\n",
    "        # Get diagnostics about what transformations will be performed\n",
    "        log_diagnostics_before = transform_skewness(df, \n",
    "                                                    min_dummies=min_dummies, \n",
    "                                                    max_dummies=max_dummies,\n",
    "                                                    ignore=v_to_ignore)\n",
    "\n",
    "        # Carry out the transformations\n",
    "        df, df_levels_old, log_diagnostics_trans = \\\n",
    "            transform_skewness(df, min_dummies=min_dummies, max_dummies=max_dummies, \n",
    "                               dry_run=False, ignore=v_to_ignore,\n",
    "                               take_log=log_diagnostics_before['all_3_quartiles_equal_both'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and predictive modeling\n",
    "### Impact of not dropping endogenous features, etc.\n",
    "As explained in the previous notebook, \n",
    "\n",
    "Apply preprocessing to each of the data frames, perform classification on each using a random forests, logistic regression, and support vector machine, and then compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Endogenous and irrelevant variables dropped: []\n",
      "Dropping variables with missing values as determined to be optimal in the data cleaning notebook\n",
      "Transformed \"time-since\" variable\n",
      "Transformed types\n",
      "Normalized monetary quantities by income\n",
      "\n",
      "Endogenous and irrelevant variables dropped: ['url', 'initial_list_status']\n",
      "Dropping variables with missing values as determined to be optimal in the data cleaning notebook\n",
      "Transformed \"time-since\" variable\n",
      "Transformed types\n",
      "Normalized monetary quantities by income\n",
      "\n",
      "Endogenous and irrelevant variables dropped: ['url', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'debt_settlement_flag', 'last_fico_range_low', 'last_fico_range_high', 'last_credit_pull_d']\n",
      "Dropping variables with missing values as determined to be optimal in the data cleaning notebook\n",
      "Transformed \"time-since\" variable\n",
      "Transformed types\n",
      "Normalized monetary quantities by income\n",
      "\n",
      "Endogenous and irrelevant variables dropped: ['url', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'debt_settlement_flag', 'last_fico_range_low', 'last_fico_range_high', 'last_credit_pull_d', 'int_rate', 'grade', 'sub_grade']\n",
      "Dropping variables with missing values as determined to be optimal in the data cleaning notebook\n",
      "Transformed \"time-since\" variable\n",
      "Transformed types\n",
      "Normalized monetary quantities by income\n",
      "\n",
      "Endogenous and irrelevant variables dropped: ['url', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'debt_settlement_flag', 'last_fico_range_low', 'last_fico_range_high', 'last_credit_pull_d', 'int_rate', 'grade', 'sub_grade', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv']\n",
      "Dropping variables with missing values as determined to be optimal in the data cleaning notebook\n",
      "Transformed \"time-since\" variable\n",
      "Transformed types\n",
      "Normalized monetary quantities by income\n"
     ]
    }
   ],
   "source": [
    "# List of names for the different models\n",
    "data_names = ['all', 'w/o irrelevant', 'w/o irrelevant & endog.',\n",
    "              'w/o irrelevant & endog. & LC',\n",
    "              'w/o irrelevant & endog. & LC & others']\n",
    "# List of keyword arguments to create data frames with different columns\n",
    "kwargs_list = [{'v_to_drop': []}, \n",
    "               {'v_to_drop': irrelevant},\n",
    "               {'v_to_drop': irrelevant + endogenous},\n",
    "               {'v_to_drop': irrelevant + endogenous + lc},\n",
    "               {'v_to_drop': irrelevant + endogenous + lc + other_to_drop}]\n",
    "\n",
    "# Dictionaries to store results\n",
    "average_precision = {} # Single evaluation metric\n",
    "classification_reports = {}\n",
    "classifiers = {} # To save classification results\n",
    "n_cols = {} # Number of observations, to check transformations worked properly\n",
    "most_important_features = {}\n",
    "\n",
    "# Create data and estimate model in a loop\n",
    "for df_name, kwarg in zip(data_names, kwargs_list):\n",
    "    \n",
    "    data = make_data(all_data, transform_skewed_v=False, **kwarg)\n",
    "    # Store a number of columns for sanity check\n",
    "    n_cols[df_name] = data.shape[1]\n",
    "    \n",
    "\n",
    "    # Preprocessing\n",
    "    # -------------\n",
    "    \n",
    "    # Find categorical variables with more than 50 unique values and drop them\n",
    "    unique_values_cat = data.select_dtypes(include='object') \\\n",
    "                            .nunique() \\\n",
    "                            .sort_values(ascending=False) \n",
    "    # Drop variables\n",
    "    data = data.drop(unique_values_cat[unique_values_cat > 50].index,\n",
    "        axis='columns')\n",
    "\n",
    "\n",
    "    # Train-test split (for now only select 1% for training)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(data.drop('default', axis='columns'),\n",
    "                         data.default,\n",
    "                         # train_size=0.8,\n",
    "                         train_size=10000, test_size=10000,\n",
    "                         random_state=1,\n",
    "                         shuffle=True, stratify=data.default) \n",
    "\n",
    "    # Imputation and standardization for numeric features\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "    numeric_transformer = Pipeline(steps =[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())]) \n",
    "\n",
    "    # Imputation and one-hot encoding for categorical features\n",
    "    categorical_features = X_train.select_dtypes(include=[object]).columns\n",
    "    categorical_transformer = Pipeline(steps =[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Combining preprocessing for both kinds of features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric_transformer', \n",
    "                 numeric_transformer, numeric_features),\n",
    "            ('categorical_transformer', \n",
    "                 categorical_transformer, categorical_features)],\n",
    "        remainder='passthrough', n_jobs=1)\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train_p = preprocessor.fit_transform(X_train)\n",
    "    X_test_p = preprocessor.transform(X_test)\n",
    "\n",
    "    \n",
    "    # Random Forests\n",
    "    # --------------\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, \n",
    "            oob_score=True, n_jobs=3, random_state=1,\n",
    "            class_weight='balanced_subsample')\n",
    "    rf.fit(X_train_p, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_rf = rf.predict(X_test_p)    \n",
    "    y_pred_proba_rf = rf.predict_proba(X_test_p)[:, 1]\n",
    "    \n",
    "    # Save results\n",
    "    average_precision[('random forests',df_name)] = \\\n",
    "        average_precision_score(y_test, y_pred_rf)\n",
    "    classification_reports[('random forests', df_name)] = \\\n",
    "        classification_report(y_test, y_pred_rf)\n",
    "    \n",
    "    # Save most important features\n",
    "    # First get a list of feature names\n",
    "    categorical_names = preprocessor \\\n",
    "        .named_transformers_['categorical_transformer'] \\\n",
    "        .named_steps['onehot'] \\\n",
    "        .get_feature_names()\n",
    "    feature_names = \\\n",
    "        list(numeric_features) + list(categorical_names)\n",
    "    # Compute feature importance and sort\n",
    "    most_important_features[('random forests', df_name)] = \\\n",
    "        pd.Series(rf.feature_importances_, index=feature_names) \\\n",
    "                .sort_values(ascending=False) \\\n",
    "                .iloc[: 10]\n",
    "\n",
    "    \n",
    "    # Logistic regression (Ridge) \n",
    "    # ---------------------------\n",
    "\n",
    "    lr = LogisticRegression(class_weight='balanced', \n",
    "                        max_iter=1000, solver='lbfgs')\n",
    "    param_grid = {'C': np.logspace(-4, 4, 5)}\n",
    "    lr_gs = GridSearchCV(lr, param_grid=param_grid, \n",
    "                         scoring=make_scorer(average_precision_score,\n",
    "                                             needs_proba=True),\n",
    "                         n_jobs=3, cv=4)\n",
    "    lr_gs.fit(X_train_p, y_train) \n",
    "\n",
    "    # Predictions\n",
    "    y_pred_lr = lr_gs.predict(X_test_p)    \n",
    "    y_pred_proba_lr = lr_gs.predict_proba(X_test_p)[:, 1]\n",
    "    \n",
    "    # Save results\n",
    "    average_precision[('logistic regression', df_name)] = \\\n",
    "        average_precision_score(y_test, y_pred_proba_lr)\n",
    "    classification_reports[('logistic regression', df_name)] = \\\n",
    "        classification_report(y_test, y_pred_lr)\n",
    "\n",
    "    # Compute feature importance and sort\n",
    "    most_important_features[('logistic regression', df_name)] = \\\n",
    "        pd.Series(lr_gs.best_estimator_.coef_[0], \n",
    "                  index=feature_names) \\\n",
    "            .sort_values(ascending=False) \\\n",
    "            .iloc[: 10] \n",
    "\n",
    "\n",
    "    # Linear SVM\n",
    "    # ----------\n",
    "    \n",
    "    svm = LinearSVC(penalty='l2', class_weight='balanced', \n",
    "              dual=False)\n",
    "    param_grid = {'C': np.logspace(-3, 3, 5)}\n",
    "    svm_gs = GridSearchCV(svm, param_grid=param_grid,\n",
    "                          scoring=make_scorer(average_precision_score),\n",
    "                                           #   needs_proba=True),\n",
    "                         n_jobs=2, cv=4)\n",
    "    svm_gs.fit(X_train_p, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_svm = svm_gs.predict(X_test_p)\n",
    "    # Distance from separating hyperplane\n",
    "    y_pred_distance_svm = svm_gs.decision_function(X_test_p)\n",
    "    \n",
    "    # Save results\n",
    "    average_precision[('SVM', df_name)] = \\\n",
    "        average_precision_score(y_test, y_pred_distance_svm)\n",
    "    classification_reports[('SVM', df_name)] = \\\n",
    "        classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random forests       all                                      0.968017\n",
       "logistic regression  all                                      0.999485\n",
       "SVM                  all                                      0.999163\n",
       "random forests       w/o irrelevant                           0.970339\n",
       "logistic regression  w/o irrelevant                           0.999460\n",
       "SVM                  w/o irrelevant                           0.999159\n",
       "random forests       w/o irrelevant & endog.                  0.215563\n",
       "logistic regression  w/o irrelevant & endog.                  0.385527\n",
       "SVM                  w/o irrelevant & endog.                  0.380564\n",
       "random forests       w/o irrelevant & endog. & LC             0.218591\n",
       "logistic regression  w/o irrelevant & endog. & LC             0.378060\n",
       "SVM                  w/o irrelevant & endog. & LC             0.372794\n",
       "random forests       w/o irrelevant & endog. & LC & others    0.214977\n",
       "logistic regression  w/o irrelevant & endog. & LC & others    0.373838\n",
       "SVM                  w/o irrelevant & endog. & LC & others    0.370135\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('random forests', 'all') :\n",
      " last_fico_range_low        0.174427\n",
      "last_fico_range_high       0.169137\n",
      "last_pymnt_amnt            0.122006\n",
      "total_rec_prncp            0.086501\n",
      "recoveries                 0.082719\n",
      "collection_recovery_fee    0.077290\n",
      "total_pymnt_inv            0.043787\n",
      "total_pymnt                0.043073\n",
      "funded_amnt                0.019046\n",
      "loan_amnt                  0.016618\n",
      "dtype: float64 \n",
      "\n",
      "('logistic regression', 'all') :\n",
      " funded_amnt                56.481623\n",
      "funded_amnt_inv            45.582298\n",
      "recoveries                 39.155220\n",
      "collection_recovery_fee    26.078198\n",
      "total_rec_int              23.821944\n",
      "loan_amnt                  22.639980\n",
      "x10_Y                       9.071286\n",
      "x6_wedding                  4.866007\n",
      "x2_D1                       4.650125\n",
      "x2_A2                       3.173689\n",
      "dtype: float64 \n",
      "\n",
      "('random forests', 'w/o irrelevant') :\n",
      " last_fico_range_low        0.166369\n",
      "last_pymnt_amnt            0.123746\n",
      "last_fico_range_high       0.122666\n",
      "total_rec_prncp            0.115889\n",
      "recoveries                 0.093711\n",
      "collection_recovery_fee    0.072756\n",
      "total_pymnt_inv            0.046773\n",
      "total_pymnt                0.045257\n",
      "funded_amnt                0.024438\n",
      "funded_amnt_inv            0.022443\n",
      "dtype: float64 \n",
      "\n",
      "('logistic regression', 'w/o irrelevant') :\n",
      " funded_amnt                56.150721\n",
      "funded_amnt_inv            45.897655\n",
      "recoveries                 39.386075\n",
      "collection_recovery_fee    26.242988\n",
      "total_rec_int              23.809609\n",
      "loan_amnt                  22.659666\n",
      "x9_Y                        8.963056\n",
      "x6_wedding                  4.869819\n",
      "x2_D1                       4.631454\n",
      "x2_A2                       3.135590\n",
      "dtype: float64 \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog.') :\n",
      " dti                      0.035111\n",
      "avg_cur_bal              0.028551\n",
      "revol_util               0.027159\n",
      "annual_inc               0.026950\n",
      "bc_open_to_buy           0.026938\n",
      "earliest_cr_line_days    0.026650\n",
      "tot_hi_cred_lim          0.026056\n",
      "bc_util                  0.025826\n",
      "mo_sin_old_rev_tl_op     0.024698\n",
      "total_bc_limit           0.024158\n",
      "dtype: float64 \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog.') :\n",
      " x6_small_business        0.234202\n",
      "x1_E                     0.222132\n",
      "x1_F                     0.216705\n",
      "term_5y                  0.205016\n",
      "dti                      0.183319\n",
      "x4_RENT                  0.150855\n",
      "acc_open_past_24mths     0.141143\n",
      "x1_G                     0.114873\n",
      "earliest_cr_line_days    0.101244\n",
      "percent_bc_gt_75         0.098636\n",
      "dtype: float64 \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog. & LC') :\n",
      " dti                      0.039030\n",
      "avg_cur_bal              0.030215\n",
      "bc_open_to_buy           0.030173\n",
      "earliest_cr_line_days    0.029667\n",
      "tot_hi_cred_lim          0.028478\n",
      "revol_util               0.028453\n",
      "annual_inc               0.028436\n",
      "bc_util                  0.027959\n",
      "mo_sin_old_rev_tl_op     0.027662\n",
      "total_bc_limit           0.026258\n",
      "dtype: float64 \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog. & LC') :\n",
      " x4_small_business                       0.266083\n",
      "term_5y                                 0.258304\n",
      "dti                                     0.210234\n",
      "x2_RENT                                 0.160321\n",
      "acc_open_past_24mths                    0.158957\n",
      "percent_bc_gt_75                        0.122264\n",
      "x0_ 60 months                           0.111981\n",
      "earliest_cr_line_days                   0.103290\n",
      "funded_amnt                             0.097499\n",
      "inv_sqrt_1p_mths_since_recent_bc_dlq    0.084295\n",
      "dtype: float64 \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog. & LC & others') :\n",
      " dti                      0.041039\n",
      "avg_cur_bal              0.032593\n",
      "bc_open_to_buy           0.031590\n",
      "annual_inc               0.031465\n",
      "earliest_cr_line_days    0.031195\n",
      "revol_util               0.031057\n",
      "tot_hi_cred_lim          0.029937\n",
      "bc_util                  0.029887\n",
      "mo_sin_old_rev_tl_op     0.029678\n",
      "total_rev_hi_lim         0.027879\n",
      "dtype: float64 \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog. & LC & others') :\n",
      " term_5y                                 0.298681\n",
      "x4_small_business                       0.280222\n",
      "dti                                     0.211608\n",
      "acc_open_past_24mths                    0.157260\n",
      "x2_RENT                                 0.155397\n",
      "x0_ 60 months                           0.129539\n",
      "percent_bc_gt_75                        0.116116\n",
      "x3_Verified                             0.102882\n",
      "earliest_cr_line_days                   0.102819\n",
      "inv_sqrt_1p_mths_since_recent_bc_dlq    0.083295\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, importance in most_important_features.items():\n",
    "    print(name, ':\\n', importance, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('random forests', 'all') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7939\n",
      "           1       1.00      0.96      0.98      2061\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     10000\n",
      "   macro avg       0.99      0.98      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      " \n",
      "\n",
      "('logistic regression', 'all') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7939\n",
      "           1       1.00      0.99      1.00      2061\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      " \n",
      "\n",
      "('SVM', 'all') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7939\n",
      "           1       0.99      1.00      0.99      2061\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      " \n",
      "\n",
      "('random forests', 'w/o irrelevant') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7939\n",
      "           1       1.00      0.96      0.98      2061\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     10000\n",
      "   macro avg       1.00      0.98      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      " \n",
      "\n",
      "('logistic regression', 'w/o irrelevant') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7939\n",
      "           1       1.00      0.99      1.00      2061\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      " \n",
      "\n",
      "('SVM', 'w/o irrelevant') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7939\n",
      "           1       0.99      1.00      0.99      2061\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      " \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog.') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      7939\n",
      "           1       0.46      0.04      0.07      2061\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.63      0.51      0.48     10000\n",
      "weighted avg       0.73      0.79      0.72     10000\n",
      " \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog.') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.66      0.75      7939\n",
      "           1       0.33      0.64      0.43      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.60      0.65      0.59     10000\n",
      "weighted avg       0.76      0.65      0.68     10000\n",
      " \n",
      "\n",
      "('SVM', 'w/o irrelevant & endog.') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.66      0.75      7939\n",
      "           1       0.33      0.64      0.43      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.60      0.65      0.59     10000\n",
      "weighted avg       0.76      0.65      0.69     10000\n",
      " \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog. & LC') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      7939\n",
      "           1       0.51      0.04      0.08      2061\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.65      0.52      0.48     10000\n",
      "weighted avg       0.74      0.79      0.72     10000\n",
      " \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog. & LC') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      7939\n",
      "           1       0.32      0.63      0.42      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.59      0.64      0.58     10000\n",
      "weighted avg       0.76      0.65      0.68     10000\n",
      " \n",
      "\n",
      "('SVM', 'w/o irrelevant & endog. & LC') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.75      7939\n",
      "           1       0.32      0.63      0.42      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.60      0.64      0.59     10000\n",
      "weighted avg       0.76      0.65      0.68     10000\n",
      " \n",
      "\n",
      "('random forests', 'w/o irrelevant & endog. & LC & others') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      7939\n",
      "           1       0.43      0.04      0.07      2061\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     10000\n",
      "   macro avg       0.62      0.51      0.48     10000\n",
      "weighted avg       0.72      0.79      0.72     10000\n",
      " \n",
      "\n",
      "('logistic regression', 'w/o irrelevant & endog. & LC & others') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      7939\n",
      "           1       0.32      0.62      0.42      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.59      0.64      0.58     10000\n",
      "weighted avg       0.76      0.65      0.68     10000\n",
      " \n",
      "\n",
      "('SVM', 'w/o irrelevant & endog. & LC & others') :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.75      7939\n",
      "           1       0.32      0.63      0.42      2061\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.60      0.64      0.58     10000\n",
      "weighted avg       0.76      0.65      0.68     10000\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, report in classification_reports.items():\n",
    "    print(name, ':\\n', report, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_pred_svm = svm.predict(X_test_p)\n",
    "plot_confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# Plot ROC curve\n",
    "y_pred_proba_svm = svm.decision_function(X_test_p)\n",
    "plot_roc(y_test, y_pred_proba_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(y_test, y_pred_proba_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def plot_confusion_matrix(y_test, y_pred, digits=3):\n",
    "#         cf = pd.DataFrame(confusion_matrix(y_test, y_pred,\n",
    "#                                           labels=[1,0]),\n",
    "#                           columns=['True', 'False'])\n",
    "#         cf.index=['True', 'False']\n",
    "#         cf.columns.name = 'Predicted'\n",
    "#         cf.index.name = 'Actual'\n",
    "#         print(round(cf / len(y_test), digits))    \n",
    "\n",
    "#     # Plot confusion matrix for random forests\n",
    "#     y_pred_rf = rf.predict(X_test_p)\n",
    "#     plot_confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "#     def plot_roc(y_test, y_pred, model_name=None):\n",
    "#         false_positive_rate, true_positive_rate, thresholds = \\\n",
    "#             roc_curve(y_test, y_pred)\n",
    "#         roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "#         plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "#                  label='AUC = {0:.3f}'.format(roc_auc))\n",
    "#         plt.legend(loc='lower right')\n",
    "#         plt.plot([0, 1],[0, 1], 'r--')\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         title='ROC Curve'\n",
    "#         # Add custom title, if specified\n",
    "#         if model_name is not None:\n",
    "#             title = ', '.join([title, model_name])\n",
    "#         plt.title(title)\n",
    "#         plt.show();\n",
    "\n",
    "#     # Plot ROC curve for random forests\n",
    "#     y_pred_proba_rf = rf.predict_proba(X_test_p)[:, 1]\n",
    "#     plot_roc(y_test, y_pred_proba_rf, 'Random Forests')\n",
    "\n",
    "\n",
    "#     def plot_precision_recall(y_test, y_pred):\n",
    "#         \"\"\"Plots precision-recall curve.\"\"\"\n",
    "\n",
    "#         average_precision = average_precision_score(y_test, y_pred)\n",
    "#         precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "#         # pdb.set_trace()\n",
    "#         step_kwargs = ({'step': 'post'}\n",
    "#                        if 'step' in signature(plt.fill_between).parameters\n",
    "#                        else {})\n",
    "#         plt.step(recall, precision, color='b', alpha=0.2,\n",
    "#                  where='post')\n",
    "#         plt.figtext(0.2, 0.2, 'Average Precision={0:0.3f}' \\\n",
    "#                                         .format(average_precision))\n",
    "#         plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.title('Precision-Recall curve')\n",
    "#         plt.show();\n",
    "\n",
    "#     # Plot precision recall curve for random forests classifier\n",
    "#     plot_precision_recall(y_test, y_pred_proba_rf)\n",
    "\n",
    "#     # Get a list of feature names\n",
    "#     cat_names = preprocessor.named_transformers_['cat'] \\\n",
    "#                     .named_steps['onehot'].get_feature_names()\n",
    "#     feature_names = list(numeric_features) + list(cat_names)\n",
    "#     # Compute feature importance and sort\n",
    "#     feature_importances = pd.Series(\n",
    "#                                 rf.feature_importances_,\n",
    "#                                 index=feature_names) \\\n",
    "#                             .sort_values(ascending=False)\n",
    "#     print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(y_test, y_pred_proba_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "573.991px",
    "left": "37px",
    "top": "66.293px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
