{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-packages-and-data\" data-toc-modified-id=\"Load-packages-and-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load packages and data</a></span></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dimensionality Reduction</a></span></li><li><span><a href=\"#Predictive-models\" data-toc-modified-id=\"Predictive-models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictive models</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-with-RBF-Kernel\" data-toc-modified-id=\"SVM-with-RBF-Kernel-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>SVM with RBF-Kernel</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperopt\" data-toc-modified-id=\"Hyperopt-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>Hyperopt</a></span></li></ul></li></ul></li><li><span><a href=\"#test\" data-toc-modified-id=\"test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>test</a></span></li><li><span><a href=\"#SVM-with-Polynomial-Kernel\" data-toc-modified-id=\"SVM-with-Polynomial-Kernel-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>SVM with Polynomial Kernel</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "import uuid\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval, pyll\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    GridSearchCV, StratifiedShuffleSplit, ShuffleSplit, \\\n",
    "    cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression, \\\n",
    "#     LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    "    precision_recall_curve, average_precision_score, f1_score, \\\n",
    "    roc_curve, auc, roc_auc_score, make_scorer,\\\n",
    "    accuracy_score, balanced_accuracy_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Boto 3 again to interact with S3 from our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boto3 resource to interact with S3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store data\n",
    "!mkdir data_processed  \n",
    "\n",
    "# Download files from S3 bucket\n",
    "bucket_name = 'lending-club-a7b2c3e3-07f7-4444-b258-5bb63c282398'\n",
    "filenames = ['X_train', 'X_test', 'y_train', 'y_test', 'feature_names']\n",
    "for filename in filenames:\n",
    "    s3.Object(bucket_name, f'{filename}.joblib') \\\n",
    "        .download_file(f'data_processed/{filename}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed training and test set, incl. feature names \n",
    "X_train = joblib.load('data_processed/X_train.joblib')\n",
    "X_test = joblib.load('data_processed/X_test.joblib')\n",
    "y_train = joblib.load('data_processed/y_train.joblib')\n",
    "y_test = joblib.load('data_processed/y_test.joblib')\n",
    "feature_names = joblib.load('data_processed/feature_names.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller subsets from data\n",
    "X_train_s, y_train_s = resample(\n",
    "    X_train, y_train, \n",
    "    replace=False, n_samples=10000, random_state=1)\n",
    "    \n",
    "X_test_s, y_test_s = resample(\n",
    "    X_test, y_test, \n",
    "    replace=False, n_samples=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to assess whether features lie in lower-dimensional subspace\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.step(range(1, len(explained_var)+2),\n",
    "         np.hstack([[0], np.cumsum(explained_var)]),\n",
    "         color='black', label='Cumulative')\n",
    "plt.bar(range(1, len(explained_var)+1),\n",
    "        explained_var, \n",
    "        label='per principal component')\n",
    "plt.legend('lowerright')\n",
    "plt.title('Explained variance for different numbers of principal component')\n",
    "plt.ylim(0,1.05)\n",
    "plt.xlabel('Number of Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with RBF-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svm_rbf_1 = SVC(kernel='rbf', probability=False, \n",
    "              cache_size=5000, class_weight='balanced')\n",
    "# Parameters to search over\n",
    "param_grid = {'C': np.logspace(-3, 3, 8),\n",
    "              'gamma': np.logspace(-5, 1, 8)}\n",
    "# Define indices for validation split (instead of proper cross-validation)\n",
    "split_indices = ShuffleSplit(n_splits=1, test_size=.2, random_state=1)\n",
    "\n",
    "svm_rbf_gs_1 = GridSearchCV(svm_rbf_1, param_grid=param_grid,\n",
    "                          return_train_score=True,\n",
    "                          scoring='average_precision', cv=split_indices)\n",
    "svm_rbf_gs_1.fit(X_train_s, y_train_s)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(svm_rbf_gs_1, 'saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "svm_rbf_gs_1 = joblib.load('saved_models/svm_rbf_gs_1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of class\n",
    "y_pred_svm_rbf_1 = svm_rbf_gs_1.predict(X_test_s)\n",
    "\n",
    "# Distance from separating hyperplane\n",
    "y_pred_distance_svm_rbf_1 = svm_rbf_gs_1.decision_function(X_test_s)\n",
    "\n",
    "# Save results\n",
    "average_precision_1['SVM (RBF Kernel)'] = \\\n",
    "    average_precision_score(y_test_s, y_pred_distance_svm_rbf_1)\n",
    "\n",
    "classification_reports_1['SVM (RBF Kernel)'] = \\\n",
    "    classification_report(y_test_s, y_pred_svm_rbf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_best_result(svm_rbf_gs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_heatmap(svm_rbf_gs_1) #, x_digits=1, x_scientific_notation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_rbf_gs_1.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto the first 30 principal component\n",
    "pca = PCA(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small\n",
    "X_train_pc = pca.fit_transform(X_train_s) \n",
    "X_test_pc = pca.transform(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data\n",
    "X_train_pc = pca.fit_transform(X_train) \n",
    "X_test_pc = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect prior distribution for C\n",
    "c_distr = hp.lognormal('C', 0, 5)\n",
    "samples = [pyll.stochastic.sample(c_distr) for i in range(1000)]\n",
    "# \n",
    "samples = list(filter(lambda x: x<100, samples))\n",
    "# Plot\n",
    "# sns.distplot(pd.Series(samples))\n",
    "pd.Series(samples).plot(kind='hist', bins=50)\n",
    "# plt.xlim([0,1000])\n",
    "plt.xlabel('c_distr')\n",
    "plt.title('Histogram for prior distribution of regularization strength')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive statistics\n",
    "print('Descriptive statistics:\\n', \n",
    "      pd.Series(samples).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect prior distribution for gamma\n",
    "gamma_distrdistr = hp.lognormal('C', np.log(1/30), 6)\n",
    "samples = [pyll.stochastic.sample(gamma_distrdistr) for i in range(1000)]\n",
    "# \n",
    "samples = list(filter(lambda x: x<100, samples))\n",
    "# Plot\n",
    "# sns.distplot(pd.Series(samples))\n",
    "pd.Series(samples).plot(kind='hist', bins=50)\n",
    "# plt.xlim([0,1000])\n",
    "plt.xlabel('gamma_distrdistr')\n",
    "plt.title('Histogram for prior distribution of regularization strength')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive statistics\n",
    "print('Descriptive statistics:\\n', \n",
    "      pd.Series(samples).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_params_svm_rbf(params):\n",
    "    \"\"\" \n",
    "    Adjust parameters where hyperopt did not allow sampling from optimal \n",
    "    distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set min or max thresholds for parameters, where applicable\n",
    "    if params['C'] >= 10:\n",
    "        params['C'] = 10\n",
    "        \n",
    "    # Return modified parameters\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(CLF, space, model_name, \n",
    "                 X_train, y_train, \n",
    "                 adjust_params=None,n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given classifier and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, CLF=CLF, \n",
    "                  progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "\n",
    "        # Adjust parameters, if specified\n",
    "        if adjust_params is not None:\n",
    "            params = adjust_params(params)\n",
    "    \n",
    "        # Instantiate CLF\n",
    "        clf = CLF(**params)\n",
    "        \n",
    "        ## Generate indices for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = StratifiedShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = StratifiedKFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(clf, X_train_pc, y_train, cv=cv,\n",
    "                                scoring='average_precision', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    # Fit the model with the optimal hyperparamters\n",
    "    clf = CLF(**best_params)\n",
    "    clf.fit(X_train_pc, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(clf, 'saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MAX_EVALS=4\n",
    "N_JOBS=3\n",
    "\n",
    "# Define search space\n",
    "space = {\n",
    "    'kernel': 'rbf',\n",
    "    'cache_size': 1500000,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 1,\n",
    "    'C': hp.lognormal('C', -3, 3),\n",
    "    'gamma': hp.lognormal('gamma', -3, 3)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters\n",
    "find_best_hp(SVC, space, model_name='svm_rbf_hp',\n",
    "              X_train=X_train_pc, y_train=y_train_s, \n",
    "              adjust_params=adjust_params_svm_rbf,\n",
    "              max_evals=MAX_EVALS,n_folds=1, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data\n",
    "MAX_EVALS=2\n",
    "N_JOBS=3\n",
    "\n",
    "# Define search space\n",
    "space = {\n",
    "    'kernel': 'rbf',\n",
    "    'cache_size': 1500000,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 1,\n",
    "    'C': hp.lognormal('C', -3, 3),\n",
    "    'gamma': hp.lognormal('gamma', -3, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.033937907488731496, 'cache_size': 1500000, 'class_weight': 'balanced', 'gamma': 0.018647589196167595, 'kernel': 'rbf', 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "# Find best hyperparameters\n",
    "find_best_hp(SVC, space, model_name='svm_rbf_hp',\n",
    "              X_train=X_train_pc, y_train=y_train_s, \n",
    "              adjust_params=adjust_params_svm_rbf,\n",
    "              max_evals=MAX_EVALS,n_folds=4, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0353898640400763, 'cache_size': 1500000, 'class_weight': 'balanced', 'gamma': 0.05422866288523594, 'kernel': 'rbf', 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "# Find best hyperparameters \n",
    "find_best_hp(SVC, space, model_name='svm_rbf_hp_2',\n",
    "              X_train=X_train_pc, y_train=y_train_s,\n",
    "              adjust_params=adjust_params_svm_rbf,  # Reuse from RF\n",
    "              max_evals=MAX_EVALS, n_jobs=N_JOBS, n_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(CLF, space, model_name, \n",
    "                 X_train, y_train, \n",
    "                 adjust_params=None,n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given classifier and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, CLF=CLF, \n",
    "                  progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "\n",
    "        # Adjust parameters, if specified\n",
    "        if adjust_params is not None:\n",
    "            params = adjust_params(params)\n",
    "    \n",
    "        # Instantiate CLF\n",
    "        clf = CLF(**params)\n",
    "        \n",
    "        ## Generate indices for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = StratifiedShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = StratifiedKFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(clf, X_train_pc, y_train, cv=cv,\n",
    "                                scoring='average_precision', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    # Fit the model with the optimal hyperparamters\n",
    "    clf = CLF(**best_params)\n",
    "    clf.fit(X_train_pc, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(clf, 'saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 2\n",
    "N_JOBS = 3\n",
    "# Define search space\n",
    "space = {\n",
    "    'kernel': 'rbf',\n",
    "    'cache_size': 1500000,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 1,\n",
    "    'C': hp.lognormal('C', -3, 3),\n",
    "    'gamma': hp.lognormal('gamma', -3, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.12154637321748384, 'cache_size': 1500000, 'class_weight': 'balanced', 'gamma': 0.009399639196726522, 'kernel': 'rbf', 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "# Find best hyperparameters \n",
    "find_best_hp(SVC, space, model_name='svm_rbf_hp_2',\n",
    "              X_train=X_train_pc, y_train=y_train_s,\n",
    "              adjust_params=adjust_params_svm_rbf,  # Reuse from RF\n",
    "              max_evals=MAX_EVALS, n_jobs=N_JOBS, n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "svm_rbf_hp = joblib.load('saved_models/svm_rbf_hp.joblib')\n",
    "\n",
    "# Calculate average precision\n",
    "def print_save_ap(clf, model_name, X_test_pc, y_test, validation_plot=False):\n",
    "    \"\"\"\n",
    "    Calculates, saves, and prints average precision score on test set; \n",
    "    optionally plot how average precision changed over iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Score classifier with the test data\n",
    "    # Try if classifier supports probability\n",
    "    try:\n",
    "        y_score = clf.predict_proba(X_test_pc)[:,1]\n",
    "    # If it doesn't, use its decision function\n",
    "    except AttributeError:\n",
    "        y_score = clf.decision_function(X_test_pc)\n",
    "   \n",
    "    # Calculate average precision\n",
    "    ap_score = average_precision_score(y_test, y_score)\n",
    "    \n",
    "    # Save AP\n",
    "    try:\n",
    "        average_precision_hp[model_name] = ap_score\n",
    "    # If dictionary to save AP doesn't exist yet, create it first\n",
    "    except NameError:\n",
    "        average_precision = {}\n",
    "        average_precision[model_name] = ap_score\n",
    "    \n",
    "    # Print AP\n",
    "    print('Best average precision score on *test* set: {}'.format(ap_score))\n",
    "    \n",
    "    \n",
    "    # Plot AP, if specified\n",
    "    if validation_plot:\n",
    "        # Load progress file with validation performance\n",
    "        progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "        progress_file = pd.read_csv(progress_file_path)\n",
    "\n",
    "        # Extract AP for each iteration\n",
    "        ap = - progress_file.loss\n",
    "        ap.plot()\n",
    "        plt.title('Performance on *Validation* Set')\n",
    "        plt.ylabel('Average Precision')\n",
    "        plt.xlabel('Iteration');\n",
    "\n",
    "print_save_ap(svm_rbf_hp, 'svm_rbf_hp', X_test_pc, y_test_s, validation_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(CLF, space, model_name,\n",
    "                 X_train, y_train, \n",
    "                 n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given classifier and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, adjust_params, CLF=CLF, progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "\n",
    "        # Instantiate CLF\n",
    "        clf = CLF(**params)\n",
    "        \n",
    "        ## Generate indices is for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = StratifiedShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = StratifiedKFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(clf, X_train, y_train, cv=cv,\n",
    "                                scoring='average_precision', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    # Fit the model with the optimal hyperparamters\n",
    "    clf = CLF(**best_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(clf, 'saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_params_svm_rbf(params):\n",
    "    \"\"\" \n",
    "    Adjust parameters where hyperopt did not allow sampling from optimal \n",
    "    distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set min or max thresholds for parameters, where applicable\n",
    "    if params['C'] >= 10:\n",
    "        params['C'] = 10\n",
    "        \n",
    "    # Return modified parameters\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(CLF, space, model_name, \n",
    "                 X_train, y_train, \n",
    "                 adjust_params=None,n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given classifier and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, CLF=CLF, \n",
    "                  progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "\n",
    "        # Adjust parameters, if specified\n",
    "        if adjust_params is not None:\n",
    "            params = adjust_params(params)\n",
    "    \n",
    "        # Instantiate CLF\n",
    "        clf = CLF(**params)\n",
    "        \n",
    "        ## Generate indices for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = StratifiedShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = StratifiedKFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(clf, X_train_pc, y_train, cv=cv,\n",
    "                                scoring='average_precision', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    # Fit the model with the optimal hyperparamters\n",
    "    clf = CLF(**best_params)\n",
    "    clf.fit(X_train_pc, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(clf, 'saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MAX_EVALS=2\n",
    "# Define search space\n",
    "space = {\n",
    "    'kernel': 'rbf',\n",
    "    'cache_size': 1500000,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 1,\n",
    "    'C': hp.loguniform('C', -3, 3),\n",
    "    'gamma': hp.loguniform('gamma', -3, 3)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters\n",
    "find_best_hp(SVC, space, model_name='svm_rbf_hp',\n",
    "              X_train=X_train_pc, y_train=y_train_s, \n",
    "              adjust_params=adjust_params_svm_rbf,\n",
    "              max_evals=MAX_EVALS,n_folds=1, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "svm_rbf_hp = joblib.load('saved_models/svm_rbf_hp.joblib')\n",
    "\n",
    "# Calculate average precision\n",
    "def print_save_ap(clf, model_name, X_test_pc, y_test, validation_plot=False):\n",
    "    \"\"\"\n",
    "    Calculates, saves, and prints average precision score on test set; \n",
    "    optionally plot how average precision changed over iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Score classifier with the test data\n",
    "    # Try if classifier supports probability\n",
    "    try:\n",
    "        y_score = clf.predict_proba(X_test_pc)[:,1]\n",
    "    # If it doesn't, use its decision function\n",
    "    except AttributeError:\n",
    "        y_score = clf.decision_function(X_test_pc)\n",
    "   \n",
    "    # Calculate average precision\n",
    "    ap_score = average_precision_score(y_test, y_score)\n",
    "    \n",
    "    # Save AP\n",
    "    try:\n",
    "        average_precision_hp[model_name] = ap_score\n",
    "    # If dictionary to save AP doesn't exist yet, create it first\n",
    "    except NameError:\n",
    "        average_precision = {}\n",
    "        average_precision[model_name] = ap_score\n",
    "    \n",
    "    # Print AP\n",
    "    print('Best average precision score on *test* set: {}'.format(ap_score))\n",
    "    \n",
    "    \n",
    "    # Plot AP, if specified\n",
    "    if validation_plot:\n",
    "        # Load progress file with validation performance\n",
    "        progress_file_path = 'hp_progress/progress_' + model_name + '.csv'\n",
    "        progress_file = pd.read_csv(progress_file_path)\n",
    "\n",
    "        # Extract AP for each iteration\n",
    "        ap = - progress_file.loss\n",
    "        ap.plot()\n",
    "        plt.title('Performance on *Validation* Set')\n",
    "        plt.ylabel('Average Precision')\n",
    "        plt.xlabel('Iteration');\n",
    "\n",
    "print_save_ap(svm_rbf_hp, 'svm_rbf_hp', X_test_pc, y_test_s, validation_plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
