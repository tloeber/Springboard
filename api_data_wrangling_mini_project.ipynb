{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will require you to pull some data from the Qunadl API. Qaundl is currently the most widely used aggregator of financial market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, you will need to register a free account on the http://www.quandl.com website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you register, you will be provided with a unique API key, that you should store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the API key as a string - according to PEP8, constants are always named in all upper case\n",
    "exec(open ('../../config.py').read ()) # Load file where API keys are stored\n",
    "API_KEY = QUANDL_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qaundl has a large number of data sources, but, unfortunately, most of them require a Premium subscription. Still, there are also a good number of free datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this mini project, we will focus on equities data from the Frankfurt Stock Exhange (FSE), which is available for free. We'll try and analyze the stock prices of a company called Carl Zeiss Meditec, which manufactures tools for eye examinations, as well as medical lasers for laser eye surgery: https://www.zeiss.com/meditec/int/home.html. The company is listed under the stock ticker AFX_X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the detailed Quandl API instructions here: https://docs.quandl.com/docs/time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a dedicated Python package for connecting to the Quandl API, we would prefer that you use the *requests* package, which can be easily downloaded using *pip* or *conda*. You can find the documentation for the package here: http://docs.python-requests.org/en/master/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apart from the *requests* package, you are encouraged to not use any third party Python packages, such as *pandas*, and instead focus on what's available in the Python Standard Library (the *collections* module might come in handy: https://pymotw.com/3/collections/).\n",
    "Also, since you won't have access to DataFrames, you are encouraged to us Python's native data structures - preferably dictionaries, though some questions can also be answered using lists.\n",
    "You can read more on these data structures here: https://docs.python.org/3/tutorial/datastructures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the JSON responses you will be getting from the API map almost one-to-one to Python's dictionaries. Unfortunately, they can be very nested, so make sure you read up on indexing dictionaries in the documentation provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the relevant modules\n",
    "import requests\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse\n",
    "# into the JSON structure that will be returned\n",
    "url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json' \\\n",
    "     '?&start_date=2018-08-21&end_date=2018-08-21&api_key={}' \\\n",
    "        .format(API_KEY)\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id :\n",
      " 10095370 \n",
      "\n",
      "dataset_code :\n",
      " AFX_X \n",
      "\n",
      "database_code :\n",
      " FSE \n",
      "\n",
      "name :\n",
      " Carl Zeiss Meditec (AFX_X) \n",
      "\n",
      "description :\n",
      " Stock Prices for Carl Zeiss Meditec (AFX) from the Frankfurt Stock Exchange.<br><br>Trading System: Xetra<br><br>ISIN: DE0005313704 \n",
      "\n",
      "refreshed_at :\n",
      " 2018-08-23T22:31:11.714Z \n",
      "\n",
      "newest_available_date :\n",
      " 2018-08-23 \n",
      "\n",
      "oldest_available_date :\n",
      " 2000-06-07 \n",
      "\n",
      "column_names :\n",
      " ['Date', 'Open', 'High', 'Low', 'Close', 'Change', 'Traded Volume', 'Turnover', 'Last Price of the Day', 'Daily Traded Units', 'Daily Turnover'] \n",
      "\n",
      "frequency :\n",
      " daily \n",
      "\n",
      "type :\n",
      " Time Series \n",
      "\n",
      "premium :\n",
      " False \n",
      "\n",
      "limit :\n",
      " None \n",
      "\n",
      "transform :\n",
      " None \n",
      "\n",
      "column_index :\n",
      " None \n",
      "\n",
      "start_date :\n",
      " 2018-08-21 \n",
      "\n",
      "end_date :\n",
      " 2018-08-21 \n",
      "\n",
      "data :\n",
      " [['2018-08-21', None, 74.55, 73.0, 73.9, None, 75511.0, 5575558.0, None, None, None]] \n",
      "\n",
      "collapse :\n",
      " None \n",
      "\n",
      "order :\n",
      " None \n",
      "\n",
      "database_id :\n",
      " 6129 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the JSON structure of the object you created, and take note of how nested it is,\n",
    "# as well as the overall structure\n",
    "\n",
    "# The top level dictionary only has one key ('dataset'), so let's focus on its value, which is another dictionary.\n",
    "for key, value in r.json()['dataset'].items(): \n",
    "    print(key, \":\\n\", value, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  These are your tasks for this mini project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?&start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY)\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Convert the returned JSON object into a Python dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_json = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Calculate what the highest and lowest opening prices were for the stock in this period.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is that the data of interest are contained in a list of lists.  To make accessing the relevant data easier, I start by converting this object into a named tuple. \n",
    "\n",
    "(Of course, it would be easiest to use a pd.Dataframe, but the instructions state that only tools from the standard library should be used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace spaces in column names and make them lowercase\n",
    "names = [name.replace(' ', '_').lower() for name in r_json['dataset']['column_names']]\n",
    "\n",
    "# Create namedtuple class enter an empty list where to store instances\n",
    "Data = namedtuple('data', names)\n",
    "data =[]\n",
    "\n",
    "# Create namedtuple out of 'dataset' (which currently is a list of list) and append to list.\n",
    "for observation in r_json['dataset']['data']:\n",
    "    data.append(Data._make(observation))\n",
    "    \n",
    "# Sort the data by ascending date\n",
    "data = sorted(data, key=lambda observation: observation.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data(date='2017-01-02', open=34.99, high=35.94, low=34.99, close=35.8, change=None, traded_volume=44700.0, turnover=1590561.0, last_price_of_the_day=None, daily_traded_units=None, daily_turnover=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest opening price was 53.11 and occurred on 2017-12-14.\n",
      "The lowest opening price was 34.0 and occurred on 2017-01-24.\n"
     ]
    }
   ],
   "source": [
    "# Create another nametuple class to store highest and lowest opening prices, \n",
    "# as well as the respective dates\n",
    "Extrema = namedtuple('Extrema', ['highest_open', 'highest_date', \n",
    "                                 'lowest_open', 'lowest_date'])\n",
    "\n",
    "# Instantiate it and fill it with the first observation\n",
    "extrema = Extrema._make([data[0].open, data[0].date, data[0].open, data[0].date])\n",
    "\n",
    "# Loop over the remaining observations to find actual opening highs and lows\n",
    "for observation in data[1:]:\n",
    "    try:\n",
    "        if observation.open > extrema.highest_open:\n",
    "            extrema = Extrema._make([observation.open, observation.date, \n",
    "                                     extrema.lowest_open, extrema.lowest_date])\n",
    "        elif observation.open < extrema.lowest_open:\n",
    "            extrema = Extrema._make([extrema.highest_open, extrema.highest_date, \n",
    "                                     observation.open, observation.date])\n",
    "    \n",
    "    # If the opening value is missing, continue to the next observation\n",
    "    except TypeError:\n",
    "        continue\n",
    "    \n",
    "print('The highest opening price was {high} and occurred on {highest_date}.\\n'\n",
    "      'The lowest opening price was {low} and occurred on {lowest_date}.'\n",
    "          .format(high=extrema.highest_open, highest_date=extrema.highest_date,\n",
    "                  low=extrema.lowest_open, lowest_date=extrema.lowest_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What was the largest change in any one day (based on High and Low price)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest change in any single day was 2.8100000000000023 and occurred on 2017-05-11.\n"
     ]
    }
   ],
   "source": [
    "# Create a nametuple to store the maximum and associated date\n",
    "Maximum_change_within = namedtuple('Maximum_change_within', ['change', 'date']) \n",
    "\n",
    "# Instantiate it by filling it with the value from the first observation:\n",
    "maximum_change_within = Maximum_change_within._make([data[0].high - data[0].low, \n",
    "                                                     data[0].date])\n",
    "\n",
    "#Loop over the remaining observations to find actual maximum\n",
    "for observation in data[1:]:\n",
    "    change = observation.high - observation.low\n",
    "    if change > maximum_change_within.change:\n",
    "        maximum_change_within = Maximum_change_within._make([change, observation.date])\n",
    "\n",
    "print('The largest change in any single day was {} and occurred on {}.'\n",
    "         .format(maximum_change_within.change, maximum_change_within.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What was the largest change between any two days (based on Closing Price)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we sorted the data by ascending date , we can simply compute the difference between adjacent observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest change between any two days(based on closing price) was 1.7199999999999989 and occurred between 2017-05-10 and 2017-05-11.\n"
     ]
    }
   ],
   "source": [
    "# Again create a name to a tuple 1st in which to store the values\n",
    "Maximum_change_between = namedtuple('Maximum_change_between',\n",
    "                                    ['change','date_begin', 'date_end'])\n",
    "\n",
    "# Instantiate it by filling it with the value from the first observation\n",
    "maximum_change_between = Maximum_change_between._make([data[1].close - data[0].close, \n",
    "                                                       data[0].date, \n",
    "                                                       data[1].date])\n",
    "\n",
    "# Loop over the remaining observations\n",
    "for i in range(1, len(data) - 1):\n",
    "    change = data[i+1].close - data[i].close\n",
    "    if change > maximum_change_between.change:\n",
    "        maximum_change_between = Maximum_change_between._make([change, \n",
    "                                                               data[i].date, \n",
    "                                                               data[i+1].date])\n",
    "    \n",
    "print('The largest change between any two days(based on closing price) '\n",
    "      'was {} and occurred between {} and {}.'\n",
    "         .format(maximum_change_between.change, maximum_change_between.date_begin, \n",
    "                 maximum_change_between.date_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What was the average daily trading volume during this year?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average daily trading volume during the year was 89124.33725490196.\n"
     ]
    }
   ],
   "source": [
    "# Store the trading volume for each day in a list\n",
    "daily_trading_volume =[]\n",
    "for observation in data:\n",
    "    daily_trading_volume.append(observation.traded_volume)\n",
    "    \n",
    "# Compute mean\n",
    "mean_trading_volume = sum(daily_trading_volume)/len(daily_trading_volume)\n",
    "\n",
    "print('The average daily trading volume during the year was {}.'\n",
    "          .format(mean_trading_volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. (Optional) What was the median trading volume during this year. (Note: you may need to implement your own function for calculating the median.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First check whether we have an even or odd number of observations\n",
    "len(daily_trading_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median trading volume during the year was 76286.0.\n"
     ]
    }
   ],
   "source": [
    "# Since it is odd, we can simply take the middle value\n",
    "# (Round down to account for zero-based indexing)\n",
    "middle_position = int(len(daily_trading_volume)/2 - .5) \n",
    "median_training_volume = sorted(daily_trading_volume)[middle_position]\n",
    "\n",
    "print('The median trading volume during the year was {}.'\n",
    "          .format(median_training_volume))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
